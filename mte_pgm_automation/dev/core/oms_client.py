"""
OMSå®¢æˆ·ç«¯æ¨¡å— - è´Ÿè´£ä¸SK Hynix OMSç³»ç»Ÿäº¤äº’
"""
import requests
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin
import time
import hashlib
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))  # scriptsç›®å½•
dev_dir = os.path.dirname(current_dir)  # devç›®å½•
project_root = os.path.dirname(dev_dir)  # é¡¹ç›®æ ¹ç›®å½•

sys.path.insert(0, dev_dir)
sys.path.insert(0, project_root)
from utils.config_loader import get_config
from utils.logger import get_pgm_logger
from database.models import PGMOmsHistory
from database.repositories import PGMOmsHistoryRepository


class OMSClient:
    """OMSç³»ç»Ÿå®¢æˆ·ç«¯"""

    def __init__(self):
        """åˆå§‹åŒ–OMSå®¢æˆ·ç«¯"""
        self.logger = get_pgm_logger().get_logger('oms')
        self.config = get_config().get_oms_config()
        self.token = None
        self.token_expiry = None

        # ETå’ŒAT APIç«¯ç‚¹
        self.et_detail_endpoint = "/bpms/test-pgm/module/dram-et/process-id"
        self.at_detail_endpoint = "/bpms/test-pgm/module/dram-at/process-id"

        # ç¼“å­˜æœ€è¿‘æŸ¥è¯¢çš„PGMæ•°æ®ï¼ˆå¯é€‰ï¼‰
        self.cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

        self.logger.info("ğŸ”§ OMSå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

    def _get_headers(self, additional_headers: Dict[str, str] = None) -> Dict[str, str]:
        headers = self.config['headers'].copy()
        """è·å–è¯·æ±‚å¤´"""
        headers.update({
            "uiId": "ModuleTestPgmDistributeStatus",
            "uiName": "BPM%20%3E%20MOD%20Test%20PGM%20Distribute%20Status",
            "sec-ch-ua-platform": "\"Windows\"",
            "sec-ch-ua": "\"Microsoft Edge\";v=\"143\", \"Chromium\";v=\"143\", \"Not A(Brand\";v=\"24\"",
            "sec-ch-ua-mobile": "?0",
            "Accept": "application/json, text/plain, */*",
            "Sec-Fetch-Site": "same-site",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Dest": "empty"
        })

        if self.token:
            headers['Authorization'] = self.token

        if additional_headers:
            headers.update(additional_headers)

        return headers

    def login(self) -> bool:
        """
        ç™»å½•OMSç³»ç»Ÿè·å–token

        Returns:
            æ˜¯å¦ç™»å½•æˆåŠŸ
        """
        try:
            # å¦‚æœtokenä»ç„¶æœ‰æ•ˆï¼Œç›´æ¥ä½¿ç”¨
            if self.token and self.token_expiry and datetime.now() < self.token_expiry:
                self.logger.debug("âœ… ä½¿ç”¨ç¼“å­˜çš„OMS token")
                return True

            login_url = urljoin(self.config['api_base'], self.config['auth_endpoint'])

            login_data = {
                "id": self.config['user_id'],
                "password": self.config['password']
            }

            self.logger.info(f"ğŸ”‘ æ­£åœ¨ç™»å½•OMSç³»ç»Ÿ: {self.config['user_id']}")

            response = requests.post(
                login_url,
                json=login_data,
                headers=self._get_headers(),
                timeout=30
            )

            response.raise_for_status()

            result = response.json()
            token = result.get("token")

            if not token:
                self.logger.error("âŒ OMSç™»å½•å¤±è´¥: å“åº”ä¸­æœªæ‰¾åˆ°token")
                return False

            self.token = f"Bearer {token}"

            # è®¾ç½®tokenè¿‡æœŸæ—¶é—´ï¼ˆå‡è®¾tokenæœ‰æ•ˆæœŸä¸º1å°æ—¶ï¼‰
            self.token_expiry = datetime.now() + timedelta(minutes=55)

            self.logger.info("âœ… OMSç™»å½•æˆåŠŸ")
            return True

        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (ç½‘ç»œé”™è¯¯): {str(e)}")
            return False
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return False
        except Exception as e:
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return False

    def _ensure_login(self) -> bool:
        """ç¡®ä¿å·²ç™»å½•"""
        if not self.token or (self.token_expiry and datetime.now() >= self.token_expiry):
            return self.login()
        return True

    def get_et_pgm_details(self, process_id: str, work_sequence: int) -> Optional[Dict[str, Any]]:
        """
        è·å–ET PGMè¯¦ç»†ä¿¡æ¯

        Args:
            process_id: æµç¨‹ID (UUIDæ ¼å¼)
            work_sequence: å·¥ä½œåºåˆ—å·

        Returns:
            ET PGMè¯¦ç»†ä¿¡æ¯
        """
        try:
            if not self._ensure_login():
                self.logger.error("âŒ è·å–ETè¯¦æƒ…å¤±è´¥: ç™»å½•æ— æ•ˆ")
                return None

            url = urljoin(self.config['api_base'], self.et_detail_endpoint)

            params = {
                "processId": process_id,
                "workSequence": work_sequence
            }

            self.logger.info(f"ğŸ“‹ è·å–ET PGMè¯¦æƒ…: process_id={process_id}, work_sequence={work_sequence}")

            start_time = time.time()
            response = requests.get(
                url,
                params=params,
                headers=self._get_headers(),
                timeout=60
            )
            response_time = time.time() - start_time

            response.raise_for_status()

            data = response.json()

            self.logger.info(f"âœ… æˆåŠŸè·å–ET PGMè¯¦æƒ…ï¼Œå“åº”æ—¶é—´: {response_time:.2f}ç§’")

            # æå–å…³é”®ä¿¡æ¯
            extracted_data = self._extract_et_info(data, process_id, work_sequence)

            return extracted_data

        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ è·å–ET PGMè¯¦æƒ…å¤±è´¥ (ç½‘ç»œé”™è¯¯): {str(e)}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ è·å–ET PGMè¯¦æƒ…å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ è·å–ET PGMè¯¦æƒ…å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return None

    def get_at_pgm_details(self, process_id: str, work_sequence: int) -> Optional[Dict[str, Any]]:
        """
        è·å–AT PGMè¯¦ç»†ä¿¡æ¯

        Args:
            process_id: æµç¨‹ID (UUIDæ ¼å¼)
            work_sequence: å·¥ä½œåºåˆ—å·

        Returns:
            AT PGMè¯¦ç»†ä¿¡æ¯
        """
        try:
            if not self._ensure_login():
                self.logger.error("âŒ è·å–ATè¯¦æƒ…å¤±è´¥: ç™»å½•æ— æ•ˆ")
                return None

            url = urljoin(self.config['api_base'], self.at_detail_endpoint)

            params = {
                "processId": process_id,
                "workSequence": work_sequence
            }

            self.logger.info(f"ğŸ“‹ è·å–AT PGMè¯¦æƒ…: process_id={process_id}, work_sequence={work_sequence}")

            start_time = time.time()
            response = requests.get(
                url,
                params=params,
                headers=self._get_headers(),
                timeout=60
            )
            response_time = time.time() - start_time

            response.raise_for_status()

            data = response.json()

            self.logger.info(f"âœ… æˆåŠŸè·å–AT PGMè¯¦æƒ…ï¼Œå“åº”æ—¶é—´: {response_time:.2f}ç§’")

            # æå–å…³é”®ä¿¡æ¯
            extracted_data = self._extract_at_info(data, process_id, work_sequence)

            return extracted_data

        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ è·å–AT PGMè¯¦æƒ…å¤±è´¥ (ç½‘ç»œé”™è¯¯): {str(e)}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ è·å–AT PGMè¯¦æƒ…å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ è·å–AT PGMè¯¦æƒ…å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return None

    def _extract_et_info(self, data: Dict[str, Any], process_id: str, work_sequence: int) -> Dict[str, Any]:
        """æå–ETä¿¡æ¯"""
        try:
            extracted = {
                'process_id': process_id,
                'work_sequence': work_sequence,
                'type': 'ET',
                'extracted_at': datetime.now().isoformat(),
                'pgm_records': [],
                'file_info': [],
                'work_info': {}
            }

            # æå–å·¥ä½œè¯¦æƒ…
            if 'workDetailViews' in data and data['workDetailViews']:
                work_detail = data['workDetailViews'][0]
                extracted['work_info'] = {
                    'process_name': work_detail.get('processName'),
                    'work_name': work_detail.get('workName'),
                    'status': work_detail.get('status'),
                    'process_start_time': work_detail.get('processStartTime'),
                    'process_end_time': work_detail.get('processEndTime'),
                    'work_start_time': work_detail.get('workStartTime'),
                    'work_end_time': work_detail.get('workEndTime'),
                    'factory': work_detail.get('factory'),
                    'specified_work_user': work_detail.get('specifiedWorkUserName')
                }

                # æå–æ–‡ä»¶ä¿¡æ¯
                if 'file' in work_detail and work_detail['file']:
                    for file_item in work_detail['file']:
                        file_info = {
                            'file_download_id': file_item.get('fileDownloadId'),
                            'file_name': file_item.get('fileName'),
                            'size': file_item.get('size')
                        }
                        extracted['file_info'].append(file_info)

            # æå–PGMè®°å½•
            if 'testProgramModuleDramEtViews' in data:
                for pgm_record in data['testProgramModuleDramEtViews']:
                    pgm_info = {
                        'draft_id': pgm_record.get('draftId'),
                        'pgm_id': pgm_record.get('pgmId'),
                        'pgm_rev_ver': pgm_record.get('pgmRevVer'),
                        'pgm_dir': pgm_record.get('pgmDir'),
                        'pgm_dir2': pgm_record.get('pgmDir2'),
                        'pgm_dir3': pgm_record.get('pgmDir3'),
                        'pgm_dir4': pgm_record.get('pgmDir4'),
                        'equipment_model_code': pgm_record.get('equipmentModelCode'),
                        'operation_id': pgm_record.get('operationId'),
                        'module_type': pgm_record.get('moduleType'),
                        'product_type': pgm_record.get('productType'),
                        'tech_nm': pgm_record.get('techNm'),
                        'pkg_den_typ': pgm_record.get('pkgDenTyp'),
                        'organiz_cd': pgm_record.get('organizCd'),
                        'den_typ': pgm_record.get('denTyp'),
                        'change_date_time': pgm_record.get('changeDateTime'),
                        'factory_id': pgm_record.get('factoryId')
                    }
                    extracted['pgm_records'].append(pgm_info)

            self.logger.info(f"ğŸ“Š æå–ETä¿¡æ¯: {len(extracted['pgm_records'])}æ¡PGMè®°å½•")
            return extracted

        except Exception as e:
            self.logger.error(f"âŒ æå–ETä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def _extract_at_info(self, data: Dict[str, Any], process_id: str, work_sequence: int) -> Dict[str, Any]:
        """æå–ATä¿¡æ¯"""
        try:
            extracted = {
                'process_id': process_id,
                'work_sequence': work_sequence,
                'type': 'AT',
                'extracted_at': datetime.now().isoformat(),
                'pgm_records': [],
                'file_info': [],
                'work_info': {}
            }

            # æå–å·¥ä½œè¯¦æƒ…
            if 'workDetailViews' in data and data['workDetailViews']:
                work_detail = data['workDetailViews'][0]
                extracted['work_info'] = {
                    'process_name': work_detail.get('processName'),
                    'work_name': work_detail.get('workName'),
                    'status': work_detail.get('status'),
                    'process_start_time': work_detail.get('processStartTime'),
                    'process_end_time': work_detail.get('processEndTime'),
                    'work_start_time': work_detail.get('workStartTime'),
                    'work_end_time': work_detail.get('workEndTime'),
                    'factory': work_detail.get('factory'),
                    'specified_work_user': work_detail.get('specifiedWorkUserName')
                }

                # æå–æ–‡ä»¶ä¿¡æ¯
                if 'file' in work_detail and work_detail['file']:
                    for file_item in work_detail['file']:
                        file_info = {
                            'file_download_id': file_item.get('fileDownloadId'),
                            'file_name': file_item.get('fileName'),
                            'size': file_item.get('size')
                        }
                        extracted['file_info'].append(file_info)

            # æå–PGMè®°å½•
            if 'testProgramModuleDramAtViews' in data:
                for pgm_record in data['testProgramModuleDramAtViews']:
                    pgm_info = {
                        'draft_id': pgm_record.get('draftId'),
                        'pgm_id': pgm_record.get('pgmId'),
                        'pgm_rev_ver': pgm_record.get('pgmRevVer'),
                        'pgm_dir': pgm_record.get('pgmDir'),
                        'hdiag_dir': pgm_record.get('hdiagDir'),
                        'test_board_id': pgm_record.get('testBoardId'),
                        'operation_id': pgm_record.get('operationId'),
                        'module_type': pgm_record.get('moduleType'),
                        'product_type': pgm_record.get('productType'),
                        'tech_nm': pgm_record.get('techNm'),
                        'pkg_den_typ': pgm_record.get('pkgDenTyp'),
                        'sap_history_code': pgm_record.get('sapHistoryCode'),
                        'temper_val': pgm_record.get('temperVal'),
                        'change_date_time': pgm_record.get('changeDateTime'),
                        'factory_id': pgm_record.get('factoryId')
                    }
                    extracted['pgm_records'].append(pgm_info)

            self.logger.info(f"ğŸ“Š æå–ATä¿¡æ¯: {len(extracted['pgm_records'])}æ¡PGMè®°å½•")
            return extracted

        except Exception as e:
            self.logger.error(f"âŒ æå–ATä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def download_file(self, file_download_id: str, save_path: str) -> bool:
        """
        ä¸‹è½½æ–‡ä»¶

        Args:
            file_download_id: æ–‡ä»¶ä¸‹è½½ID
            save_path: ä¿å­˜è·¯å¾„

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            if not self._ensure_login():
                self.logger.error("âŒ ä¸‹è½½æ–‡ä»¶å¤±è´¥: ç™»å½•æ— æ•ˆ")
                return False

            # å‡è®¾ä¸‹è½½æ–‡ä»¶çš„APIç«¯ç‚¹ï¼ˆéœ€è¦ç¡®è®¤å®é™…APIï¼‰
            download_url = f"{self.config['api_base']}/files/download/{file_download_id}"

            self.logger.info(f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {file_download_id} -> {save_path}")

            response = requests.get(
                download_url,
                headers=self._get_headers(),
                stream=True,
                timeout=120
            )

            response.raise_for_status()

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)

            # ä¸‹è½½æ–‡ä»¶
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            file_size = os.path.getsize(save_path)
            self.logger.info(f"âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ: {save_path} ({file_size} bytes)")
            return True

        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ä¸‹è½½æ–‡ä»¶å¤±è´¥ (ç½‘ç»œé”™è¯¯): {str(e)}")
            return False
        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è½½æ–‡ä»¶å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return False

class OMSDataProcessor:
    """OMSæ•°æ®å¤„ç†å™¨"""

    # work_type_desc åˆ° work_type_no çš„æ˜ å°„
    WORK_TYPE_MAP = {
        '[1Step] ê¸°ì•ˆ': 1,
        '[2Step] ì™¸ì£¼ì‚¬ ê²°ê³¼': 2,
        '[3Step] ìµœì¢…ìŠ¹ì¸': 3,
        '[4Step] ì–‘ì‚°ì ìš©': 4
    }

    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨"""
        self.logger = get_pgm_logger().get_logger('oms_processor')
        self.oms_client = OMSClient()
        self.repository = PGMOmsHistoryRepository()

    def fetch_and_process_latest_data(self) -> bool:
        """
        è·å–å¹¶å¤„ç†æœ€æ–°çš„OMSæ•°æ®

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸ”„ å¼€å§‹è·å–æœ€æ–°OMSæ•°æ®")

            # 1. è·å–æœ€æ–°æ•°æ®ï¼ˆé»˜è®¤è·å–11å¤©å†…çš„æ•°æ®ï¼‰
            pgm_data = self.oms_client.get_pgm_distribution_status()

            if not pgm_data:
                self.logger.warning("âš ï¸ æœªè·å–åˆ°OMSæ•°æ®")
                return False

            self.logger.info(f"ğŸ“Š è·å–åˆ° {len(pgm_data)} æ¡OMSè®°å½•")

            # 2. å¤„ç†æ¯æ¡è®°å½•
            success_count = 0
            error_count = 0

            for item in pgm_data:
                try:
                    processed_data = self._process_single_record(item)

                    if processed_data:
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        if self.repository.upsert_oms_record(processed_data):
                            success_count += 1
                        else:
                            error_count += 1
                    else:
                        error_count += 1

                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†OMSè®°å½•å¤±è´¥: {str(e)}")
                    error_count += 1

            # 3. è®°å½•ç»“æœ
            self.logger.info(f"âœ… OMSæ•°æ®å¤„ç†å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")

            return success_count > 0

        except Exception as e:
            self.logger.error(f"âŒ OMSæ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
            return False

    def _process_single_record(self, raw_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•æ¡OMSè®°å½•

        Args:
            raw_data: åŸå§‹æ•°æ®

        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        try:
            # æå–å…³é”®å­—æ®µ
            draft_id = raw_data.get('draftId')
            work_type_desc = raw_data.get('workTypeDesc')

            if not draft_id or not work_type_desc:
                self.logger.warning(f"âš ï¸ OMSè®°å½•ç¼ºå°‘å…³é”®å­—æ®µ: {raw_data}")
                return None

            # æ˜ å°„ work_type_no
            work_type_no = self.WORK_TYPE_MAP.get(work_type_desc)

            # æ„å»ºå¤„ç†åçš„æ•°æ®
            processed_data = {
                'draft_id': draft_id,
                'work_type_desc': work_type_desc,
                'process_id': raw_data.get('processId'),
                'work_type_no': work_type_no,
                'work_status': raw_data.get('workStatus'),
                'work_start_tm': raw_data.get('workStartTm'),
                'complete_yn': raw_data.get('completeYn'),
                'user_id': raw_data.get('userId'),
                'user_name': raw_data.get('userName'),
                'fac_id': raw_data.get('facId'),
                'process_name': raw_data.get('processName'),
                'process_status_code': raw_data.get('processStatusCode')
            }

            # è®¡ç®—TATç›¸å…³ä¿¡æ¯
            if work_type_no and raw_data.get('workStartTm'):
                tat_info = self._calculate_tat_info(
                    raw_data['workStartTm'],
                    work_type_no
                )
                processed_data.update(tat_info)

            return processed_data

        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†å•æ¡OMSè®°å½•å¤±è´¥: {str(e)}")
            return None

    def _calculate_tat_info(self, work_start_tm: str, work_type_no: int) -> Dict[str, Any]:
        """
        è®¡ç®—TATç›¸å…³ä¿¡æ¯

        Args:
            work_start_tm: å·¥ä½œå¼€å§‹æ—¶é—´
            work_type_no: å·¥ä½œç±»å‹ç¼–å·

        Returns:
            TATä¿¡æ¯å­—å…¸
        """
        try:
            # è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼ˆæ ¼å¼å¯èƒ½ä¸º "MM/DD HH:MM" æˆ– "YYYY/MM/DD HH:MM"ï¼‰
            try:
                # å°è¯•æ ¼å¼ "YYYY/MM/DD HH:MM"
                if '/' in work_start_tm and ':' in work_start_tm:
                    parts = work_start_tm.split()
                    if len(parts) == 2:
                        date_part, time_part = parts
                        date_parts = date_part.split('/')

                        if len(date_parts) == 2:  # "MM/DD"
                            # æ·»åŠ å½“å‰å¹´ä»½
                            current_year = datetime.now().year
                            start_time = datetime.strptime(
                                f"{current_year}/{date_part} {time_part}",
                                "%Y/%m/%d %H:%M"
                            )
                        elif len(date_parts) == 3:  # "YYYY/MM/DD"
                            start_time = datetime.strptime(work_start_tm, "%Y/%m/%d %H:%M")
                        else:
                            start_time = datetime.now()
                    else:
                        start_time = datetime.now()
                else:
                    start_time = datetime.now()
            except:
                start_time = datetime.now()

            # è®¡ç®—TATï¼ˆå¤©æ•°ï¼‰
            tat_days = (datetime.now() - start_time).total_seconds() / (24 * 3600)
            tat_days = round(tat_days, 2)

            # è®¡ç®—marking
            tat_marking = self._calculate_tat_marking(tat_days)

            # è®¡ç®—info_object
            info_object = "Hitech" if work_type_no in [2, 4] else "All"

            return {
                'tat_days': tat_days,
                'tat_marking': tat_marking,
                'info_object': info_object
            }

        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—TATä¿¡æ¯å¤±è´¥ ({work_start_tm}): {str(e)}")
            return {}

    def _calculate_tat_marking(self, tat_days: float) -> str:
        """
        æ ¹æ®TATè®¡ç®—marking

        Args:
            tat_days: TATå¤©æ•°

        Returns:
            markingå­—ç¬¦ä¸²
        """
        try:
            config = get_config().get_email_config()
            thresholds = config.get('tat_thresholds', {})

            alarm_hours = thresholds.get('alarm_hours', 72)
            warning_hours = thresholds.get('warning_hours', 48)
            notice_hours = thresholds.get('notice_hours', 24)

            tat_hours = tat_days * 24

            if tat_hours > alarm_hours:
                return "Alarm"
            elif tat_hours > warning_hours:
                return "Warning"
            elif tat_hours > notice_hours:
                return "Notice"
            else:
                return "Normal"

        except:
            # é»˜è®¤é˜ˆå€¼
            if tat_days > 3:
                return "Alarm"
            elif tat_days > 2:
                return "Warning"
            elif tat_days > 1:
                return "Notice"
            else:
                return "Normal"

    def check_tat_alarms(self) -> List[Dict[str, Any]]:
        """
        æ£€æŸ¥TATè¶…æ—¶å¹¶è¿”å›æŠ¥è­¦ä¿¡æ¯

        Returns:
            æŠ¥è­¦ä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info("ğŸ” æ£€æŸ¥TATè¶…æ—¶æŠ¥è­¦")

            # è·å–æœ€è¿‘çš„OMSè®°å½•
            recent_records = self.repository.get_recent_drafts(days=30)

            alarms = []
            for record in recent_records:
                try:
                    # è®¡ç®—TAT
                    if record.work_start_tm and record.work_type_no:
                        tat_info = self._calculate_tat_info(
                            record.work_start_tm,
                            record.work_type_no
                        )

                        tat_marking = tat_info.get('tat_marking', 'Normal')

                        # å¦‚æœæ˜¯Alarmçº§åˆ«ï¼Œç”ŸæˆæŠ¥è­¦ä¿¡æ¯
                        if tat_marking == 'Alarm':
                            alarm_info = {
                                'draft_id': record.draft_id,
                                'work_type_desc': record.work_type_desc,
                                'work_start_tm': record.work_start_tm,
                                'tat_days': tat_info.get('tat_days'),
                                'tat_marking': tat_marking,
                                'info_object': tat_info.get('info_object'),
                                'user_name': record.user_name,
                                'process_name': record.process_name
                            }
                            alarms.append(alarm_info)

                except Exception as e:
                    self.logger.error(f"âŒ æ£€æŸ¥å•æ¡è®°å½•TATå¤±è´¥: {str(e)}")
                    continue

            self.logger.info(f"âš ï¸ å‘ç° {len(alarms)} ä¸ªTATè¶…æ—¶æŠ¥è­¦")

            return alarms

        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥TATæŠ¥è­¦å¤±è´¥: {str(e)}")
            return []


def test_oms_client():
    """æµ‹è¯•OMSå®¢æˆ·ç«¯"""
    print("=" * 60)
    print("OMSå®¢æˆ·ç«¯æµ‹è¯•")
    print("=" * 60)

    logger = get_pgm_logger()
    logger.log_execution_start("OMSå®¢æˆ·ç«¯æµ‹è¯•")

    try:
        # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
        oms_client = OMSClient()

        # 2. æµ‹è¯•ç™»å½•
        print("ğŸ”‘ æµ‹è¯•OMSç™»å½•...")
        if oms_client.login():
            print("âœ… OMSç™»å½•æˆåŠŸ")
        else:
            print("âŒ OMSç™»å½•å¤±è´¥")
            return False

        # 3. æµ‹è¯•è·å–æ•°æ®
        print("ğŸ“‹ æµ‹è¯•è·å–PGMåˆ†å‘çŠ¶æ€...")
        pgm_data = oms_client.get_pgm_distribution_status()

        if pgm_data:
            print(f"âœ… æˆåŠŸè·å– {len(pgm_data)} æ¡PGMæ•°æ®")

            # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®
            for i, item in enumerate(pgm_data[:3]):
                print(f"  {i + 1}. {item.get('draftId', 'N/A')} - {item.get('processName', 'N/A')}")
        else:
            print("âŒ è·å–PGMæ•°æ®å¤±è´¥")
            return False

        # 4. æµ‹è¯•æ•°æ®å¤„ç†
        print("ğŸ”„ æµ‹è¯•æ•°æ®å¤„ç†...")
        processor = OMSDataProcessor()

        if processor.fetch_and_process_latest_data():
            print("âœ… OMSæ•°æ®å¤„ç†æˆåŠŸ")
        else:
            print("âŒ OMSæ•°æ®å¤„ç†å¤±è´¥")

        # 5. æµ‹è¯•TATæŠ¥è­¦æ£€æŸ¥
        print("ğŸ” æµ‹è¯•TATæŠ¥è­¦æ£€æŸ¥...")
        alarms = processor.check_tat_alarms()

        if alarms:
            print(f"âš ï¸ å‘ç° {len(alarms)} ä¸ªTATæŠ¥è­¦:")
            for alarm in alarms[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"  - {alarm['draft_id']}: TAT {alarm['tat_days']}å¤©")
        else:
            print("âœ… æœªå‘ç°TATæŠ¥è­¦")

        logger.log_execution_end("OMSå®¢æˆ·ç«¯æµ‹è¯•", success=True)
        return True

    except Exception as e:
        logger.log_execution_end("OMSå®¢æˆ·ç«¯æµ‹è¯•", success=False, message=str(e))
        print(f"âŒ OMSå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


if __name__ == "__main__":
    test_oms_client()