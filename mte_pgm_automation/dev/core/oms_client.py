"""
OMSå®¢æˆ·ç«¯æ¨¡å— - è´Ÿè´£ä¸SK Hynix OMSç³»ç»Ÿäº¤äº’
"""
import requests
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from urllib.parse import urljoin
import time
import hashlib
import sys
import os
from pathlib import Path
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))  # scriptsç›®å½•
dev_dir = os.path.dirname(current_dir)  # devç›®å½•
project_root = os.path.dirname(dev_dir)  # é¡¹ç›®æ ¹ç›®å½•

sys.path.insert(0, dev_dir)
sys.path.insert(0, project_root)
from utils.config_loader import get_config
from utils.logger import get_pgm_logger
from database.models import PGMOmsHistory
from database.repositories import PGMOmsHistoryRepository


class OMSClient:
    """OMSç³»ç»Ÿå®¢æˆ·ç«¯"""

    def __init__(self):
        """åˆå§‹åŒ–OMSå®¢æˆ·ç«¯"""
        self.logger = get_pgm_logger().get_logger('oms')
        self.config = get_config().get_oms_config()

        # è·å–ç«¯ç‚¹é…ç½®
        self.endpoints = self.config['endpoints']
        self.api_base = self.config['api_base']

        # è®¤è¯ä¿¡æ¯
        config_loader = get_config()
        env = config_loader.get_current_environment()
        if env in self.config.get('credentials', {}):
            self.credentials = self.config['credentials'][env]
        else:
            # å›é€€åˆ°é»˜è®¤å‡­æ®ç»“æ„ï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
            self.credentials = self.config.get('credentials', {})

        # Tokenç®¡ç†
        self.token = None
        self.token_expiry = None
        self.token_refresh_buffer = timedelta(minutes=5)  # æå‰5åˆ†é’Ÿåˆ·æ–°token

        # è¯·æ±‚è¶…æ—¶é…ç½®
        self.timeouts = self.config.get('request_timeout', {
            'login': 30,
            'list': 60,
            'detail': 60,
            'download': 300
        })

        # é‡è¯•é…ç½®
        self.retry_settings = self.config.get('retry_settings', {
            'max_retries': 3,
            'retry_delay_seconds': 5,
            'retry_on_status_codes': [408, 429, 500, 502, 503, 504]
        })

        # ç¼“å­˜é…ç½®
        self.cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

        # åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session
        self.session = self._create_retry_session()

        self.logger.info(f"ğŸ”§ OMSå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œç¯å¢ƒ: {env}")

        self.file_paths = get_config().get_file_paths()

    def _create_retry_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session"""
        session = requests.Session()

        retry_strategy = Retry(
            total=self.retry_settings['max_retries'],
            backoff_factor=self.retry_settings['retry_delay_seconds'],
            status_forcelist=self.retry_settings['retry_on_status_codes']
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)

        return session

    def _get_headers(self, endpoint_name: str = None) -> Dict[str, str]:
        """
        è·å–è¯·æ±‚å¤´

        Args:
            endpoint_name: ç«¯ç‚¹åç§°ï¼Œç”¨äºè·å–ç‰¹å®šç«¯ç‚¹çš„headers

        Returns:
            è¯·æ±‚å¤´å­—å…¸
        """
        headers = self.config['headers']['common'].copy()

        # æ·»åŠ ç‰¹å®šç«¯ç‚¹çš„headers
        if endpoint_name and 'endpoint_specific' in self.config['headers']:
            specific_headers = self.config['headers']['endpoint_specific'].get(endpoint_name, {})
            headers.update(specific_headers)

        # æ·»åŠ Authorizationå¤´
        if self.token and self._is_token_valid():
            headers['Authorization'] = self.token

        return headers

    def login(self, force: bool = False) -> Tuple[bool, str]:
        """
        ç™»å½•OMSç³»ç»Ÿè·å–token

        Args:
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç™»å½•

        Returns:
            (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æ£€æŸ¥tokenæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            if not force and self._is_token_valid():
                self.logger.debug("âœ… ä½¿ç”¨ç¼“å­˜çš„OMS token")
                return True, "ä½¿ç”¨ç¼“å­˜çš„token"

            login_url = urljoin(self.api_base, self.endpoints['auth'])

            # å‡†å¤‡ç™»å½•æ•°æ®
            login_data = {
                "id": self.credentials['user_id'],
                "password": self.credentials['password']
            }

            self.logger.info(f"ğŸ”‘ æ­£åœ¨ç™»å½•OMSç³»ç»Ÿ: {self.credentials['user_id']}")
            self.logger.debug(f"ç™»å½•URL: {login_url}")

            # å‘é€ç™»å½•è¯·æ±‚
            response = self.session.post(
                login_url,
                json=login_data,
                headers=self._get_headers(),
                timeout=self.timeouts['login']
            )

            response.raise_for_status()

            result = response.json()

            # æ£€æŸ¥å“åº”ç»“æ„
            if not isinstance(result, dict):
                return False, "ç™»å½•å“åº”æ ¼å¼é”™è¯¯"

            token = result.get("token")
            if not token:
                error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥: {error_msg}")
                return False, f"å“åº”ä¸­æœªæ‰¾åˆ°token: {error_msg}"

            # æ›´æ–°token
            self.token = f"Bearer {token}"

            # è§£ætokenè¿‡æœŸæ—¶é—´ï¼ˆä»å“åº”ä¸­è·å–æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            token_duration = result.get("expires_in", 3600)  # é»˜è®¤1å°æ—¶
            self.token_expiry = datetime.now() + timedelta(seconds=token_duration)

            self.logger.info(f"âœ… OMSç™»å½•æˆåŠŸï¼Œtokenæœ‰æ•ˆæœŸè‡³: {self.token_expiry}")
            return True, "ç™»å½•æˆåŠŸ"

        except requests.exceptions.Timeout:
            self.logger.error("âŒ OMSç™»å½•å¤±è´¥: è¯·æ±‚è¶…æ—¶")
            return False, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            self.logger.error("âŒ OMSç™»å½•å¤±è´¥: è¿æ¥é”™è¯¯")
            return False, "è¿æ¥é”™è¯¯"
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else "æœªçŸ¥"
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (HTTP {status_code}): {str(e)}")
            return False, f"HTTPé”™è¯¯: {status_code}"
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return False, "JSONè§£æé”™è¯¯"
        except Exception as e:
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return False, f"æœªçŸ¥é”™è¯¯: {str(e)}"

    def _is_token_valid(self) -> bool:
        """æ£€æŸ¥tokenæ˜¯å¦æœ‰æ•ˆ"""
        if not self.token or not self.token_expiry:
            return False

        # è€ƒè™‘åˆ·æ–°ç¼“å†²æ—¶é—´
        return datetime.now() < (self.token_expiry - self.token_refresh_buffer)

    def _ensure_login(self) -> bool:
        """ç¡®ä¿å·²ç™»å½•ï¼Œè‡ªåŠ¨å¤„ç†tokenåˆ·æ–°"""
        if not self._is_token_valid():
            success, message = self.login()
            if not success:
                self.logger.error(f"âŒ è‡ªåŠ¨ç™»å½•å¤±è´¥: {message}")
                return False
        return True

    def get_pgm_distribution_status(self,
                                    begin_date: Optional[str] = None,
                                    end_date: Optional[str] = None,
                                    factory_id: str = "OSMOD",
                                    company_id: str = "HITECH",
                                    force_refresh: bool = False) -> List[Dict[str, Any]]:
        """
        è·å–PGMåˆ†å‘çŠ¶æ€

        Args:
            begin_date: å¼€å§‹æ—¥æœŸ (æ ¼å¼: "YYYY-MM-DD HH:MM:SS")
            end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: "YYYY-MM-DD HH:MM:SS")
            factory_id: å·¥å‚ID
            company_id: å…¬å¸ID
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜

        Returns:
            PGMåˆ†å‘çŠ¶æ€åˆ—è¡¨
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.md5(
            f"distribution_{factory_id}_{company_id}_{begin_date}_{end_date}".encode()
        ).hexdigest()

        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and cache_key in self.cache:
            cache_data, cache_time = self.cache[cache_key]
            if time.time() - cache_time < self.cache_ttl:
                self.logger.debug(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„PGMåˆ†å‘çŠ¶æ€æ•°æ®")
                return cache_data

        try:
            # ç¡®ä¿å·²ç™»å½•
            if not self._ensure_login():
                self.logger.error("âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥: ç™»å½•æ— æ•ˆ")
                return []

            # è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´
            if not begin_date or not end_date:
                today = datetime.now()
                begin_date = (today - timedelta(days=11)).strftime("%Y-%m-%d 07:00:00")
                end_date = (today + timedelta(days=1)).strftime("%Y-%m-%d 07:00:00")

            # æ„å»ºURLå’Œå‚æ•°
            url = urljoin(self.api_base, self.endpoints['distribute_status'])

            params = {
                "factoryId": factory_id,
                "companyId": company_id,
                "beginDate": begin_date,
                "endDate": end_date
            }

            self.logger.info(f"ğŸ“‹ è·å–PGMåˆ†å‘çŠ¶æ€: {begin_date} åˆ° {end_date}")
            self.logger.debug(f"è¯·æ±‚URL: {url}")
            self.logger.debug(f"è¯·æ±‚å‚æ•°: {params}")

            # å‘é€è¯·æ±‚
            start_time = time.time()
            response = self.session.get(
                url,
                params=params,
                headers=self._get_headers('distribute_status'),
                timeout=self.timeouts['list']
            )
            response_time = time.time() - start_time

            response.raise_for_status()

            data = response.json()

            if not isinstance(data, list):
                self.logger.error(f"âŒ å“åº”æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨ï¼Œå¾—åˆ°: {type(data)}")
                return []

            self.logger.info(f"âœ… æˆåŠŸè·å–PGMåˆ†å‘çŠ¶æ€ï¼Œå…±{len(data)}æ¡è®°å½•")
            self.logger.debug(f"å“åº”æ—¶é—´: {response_time:.2f}ç§’")

            # ç¼“å­˜ç»“æœ
            self.cache[cache_key] = (data, time.time())

            return data

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else "æœªçŸ¥"

            if status_code in [401, 403]:
                self.logger.info("ğŸ”„ Tokenè¿‡æœŸæˆ–æ— æ•ˆï¼Œå°è¯•é‡æ–°ç™»å½•...")
                self.token = None
                self.token_expiry = None

                # é‡æ–°ç™»å½•å¹¶é‡è¯•
                if self._ensure_login():
                    return self.get_pgm_distribution_status(
                        begin_date, end_date, factory_id, company_id, True
                    )

            self.logger.error(f"âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥ (HTTP {status_code}): {str(e)}")
            return []
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥ (ç½‘ç»œé”™è¯¯): {str(e)}")
            return []
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return []
        except Exception as e:
            self.logger.error(f"âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return []

    def get_new_pgms(self, last_check_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """
        è·å–æ–°çš„PGMï¼ˆè‡ªä¸Šæ¬¡æ£€æŸ¥åæ–°å¢çš„ï¼‰

        Args:
            last_check_time: ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´

        Returns:
            æ–°çš„PGMåˆ—è¡¨
        """
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šä¸Šæ¬¡æ£€æŸ¥æ—¶é—´ï¼Œé»˜è®¤æ£€æŸ¥è¿‡å»1å¤©çš„æ•°æ®
            if not last_check_time:
                last_check_time = datetime.now() - timedelta(days=1)

            begin_date = last_check_time.strftime("%Y-%m-%d %H:%M:%S")
            end_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            all_pgms = self.get_pgm_distribution_status(begin_date, end_date)

            # è¿‡æ»¤å‡ºæ–°çš„PGMï¼ˆè¿™é‡Œå¯ä»¥æ ¹æ®ä¸šåŠ¡é€»è¾‘è¿›ä¸€æ­¥è¿‡æ»¤ï¼‰
            new_pgms = []
            for pgm in all_pgms:
                # ç¤ºä¾‹è¿‡æ»¤é€»è¾‘ï¼šåªè·å–ç‰¹å®šçŠ¶æ€æˆ–ç±»å‹çš„PGM
                work_type_desc = pgm.get('workTypeDesc', '')
                complete_yn = pgm.get('completeYn', '')

                # å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´è¿‡æ»¤æ¡ä»¶
                if 'PGM' in str(pgm.get('processName', '')).upper():
                    new_pgms.append(pgm)

            self.logger.info(f"ğŸ“Š å‘ç° {len(new_pgms)} ä¸ªæ–°çš„PGM")

            return new_pgms

        except Exception as e:
            self.logger.error(f"âŒ è·å–æ–°PGMå¤±è´¥: {str(e)}")
            return []

    def get_file_info_from_pgm(self, pgm_record: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»PGMè®°å½•ä¸­æå–æ–‡ä»¶ä¿¡æ¯

        Args:
            pgm_record: PGMåˆ†å‘çŠ¶æ€è®°å½•

        Returns:
            æ–‡ä»¶ä¿¡æ¯å­—å…¸
        """
        file_info = {
            'draft_id': pgm_record.get('draftId'),
            'process_id': pgm_record.get('processId'),
            'process_name': pgm_record.get('processName'),
            'work_sequence': pgm_record.get('workSequence'),
            'user_name': pgm_record.get('userName'),
            'work_start_tm': pgm_record.get('workStartTm'),
            'complete_yn': pgm_record.get('completeYn'),
            'pgm_type': self._determine_pgm_type(pgm_record)
        }

        # çŒœæµ‹å¯èƒ½çš„æ–‡ä»¶ç±»å‹
        process_name = str(pgm_record.get('processName', '')).upper()
        if 'ET' in process_name:
            file_info['expected_extensions'] = ['.obj', '.zip']
        elif 'AT' in process_name:
            file_info['expected_extensions'] = ['.xml', '.zip']
        else:
            file_info['expected_extensions'] = ['.zip', '.xml', '.obj']

        return file_info

    def _determine_pgm_type(self, pgm_record: Dict[str, Any]) -> str:
        """ç¡®å®šPGMç±»å‹"""
        process_name = str(pgm_record.get('processName', '')).upper()

        if 'ET' in process_name:
            return 'ET'
        elif 'AT' in process_name:
            return 'AT'
        elif 'HESS' in process_name:
            return 'HESS'
        else:
            return 'UNKNOWN'

    def get_et_pgm_details(self,
                           process_id: str,
                           work_sequence: int) -> Optional[Dict[str, Any]]:
        """
        è·å–ET PGMè¯¦ç»†ä¿¡æ¯

        Args:
            process_id: æµç¨‹ID (UUIDæ ¼å¼)
            work_sequence: å·¥ä½œåºåˆ—å·

        Returns:
            ET PGMè¯¦ç»†ä¿¡æ¯
        """
        return self._get_pgm_details('et', process_id, work_sequence)

    def get_at_pgm_details(self,
                           process_id: str,
                           work_sequence: int) -> Optional[Dict[str, Any]]:
        """
        è·å–AT PGMè¯¦ç»†ä¿¡æ¯

        Args:
            process_id: æµç¨‹ID (UUIDæ ¼å¼)
            work_sequence: å·¥ä½œåºåˆ—å·

        Returns:
            AT PGMè¯¦ç»†ä¿¡æ¯
        """
        return self._get_pgm_details('at', process_id, work_sequence)

    def _get_pgm_details(self,
                         pgm_type: str,
                         process_id: str,
                         work_sequence: int) -> Optional[Dict[str, Any]]:
        """
        é€šç”¨æ–¹æ³•è·å–PGMè¯¦æƒ…

        Args:
            pgm_type: 'et' æˆ– 'at'
            process_id: æµç¨‹ID
            work_sequence: å·¥ä½œåºåˆ—å·

        Returns:
            PGMè¯¦ç»†ä¿¡æ¯
        """
        try:
            if not self._ensure_login():
                self.logger.error(f"âŒ è·å–{pgm_type.upper()}è¯¦æƒ…å¤±è´¥: ç™»å½•æ— æ•ˆ")
                return None

            # æ„å»ºURL
            endpoint_key = f"{pgm_type}_detail"
            url = urljoin(self.api_base, self.endpoints[endpoint_key])

            params = {
                "processId": process_id,
                "workSequence": work_sequence
            }

            self.logger.info(
                f"ğŸ“‹ è·å–{pgm_type.upper()} PGMè¯¦æƒ…: process_id={process_id}, work_sequence={work_sequence}")

            # å‘é€è¯·æ±‚
            start_time = time.time()
            response = self.session.get(
                url,
                params=params,
                headers=self._get_headers(endpoint_key),
                timeout=self.timeouts['detail']
            )
            response_time = time.time() - start_time

            response.raise_for_status()

            data = response.json()

            self.logger.info(f"âœ… æˆåŠŸè·å–{pgm_type.upper()} PGMè¯¦æƒ…ï¼Œå“åº”æ—¶é—´: {response_time:.2f}ç§’")

            # æå–å…³é”®ä¿¡æ¯
            if pgm_type == 'et':
                extracted_data = self._extract_et_info(data, process_id, work_sequence)
            else:
                extracted_data = self._extract_at_info(data, process_id, work_sequence)

            return extracted_data

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else "æœªçŸ¥"
            self.logger.error(f"âŒ è·å–{pgm_type.upper()} PGMè¯¦æƒ…å¤±è´¥ (HTTP {status_code}): {str(e)}")
            return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ è·å–{pgm_type.upper()} PGMè¯¦æƒ…å¤±è´¥ (ç½‘ç»œé”™è¯¯): {str(e)}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ è·å–{pgm_type.upper()} PGMè¯¦æƒ…å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ è·å–{pgm_type.upper()} PGMè¯¦æƒ…å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return None

    def _extract_et_info(self, data: Dict[str, Any], process_id: str, work_sequence: int) -> Dict[str, Any]:
        """æå–ETä¿¡æ¯"""
        try:
            extracted = {
                'process_id': process_id,
                'work_sequence': work_sequence,
                'type': 'ET',
                'extracted_at': datetime.now().isoformat(),
                'pgm_records': [],
                'file_info': [],
                'work_info': {}
            }

            # æå–å·¥ä½œè¯¦æƒ…
            if 'workDetailViews' in data and data['workDetailViews']:
                work_detail = data['workDetailViews'][0]
                extracted['work_info'] = {
                    'process_name': work_detail.get('processName'),
                    'work_name': work_detail.get('workName'),
                    'status': work_detail.get('status'),
                    'process_start_time': work_detail.get('processStartTime'),
                    'process_end_time': work_detail.get('processEndTime'),
                    'work_start_time': work_detail.get('workStartTime'),
                    'work_end_time': work_detail.get('workEndTime'),
                    'factory': work_detail.get('factory'),
                    'specified_work_user': work_detail.get('specifiedWorkUserName')
                }

                # æå–æ–‡ä»¶ä¿¡æ¯
                if 'file' in work_detail and work_detail['file']:
                    for file_item in work_detail['file']:
                        file_info = {
                            'file_download_id': file_item.get('fileDownloadId'),
                            'file_name': file_item.get('fileName'),
                            'size': file_item.get('size')
                        }
                        extracted['file_info'].append(file_info)

            # æå–PGMè®°å½•
            if 'testProgramModuleDramEtViews' in data:
                for pgm_record in data['testProgramModuleDramEtViews']:
                    pgm_info = {
                        'draft_id': pgm_record.get('draftId'),
                        'pgm_id': pgm_record.get('pgmId'),
                        'pgm_rev_ver': pgm_record.get('pgmRevVer'),
                        'pgm_dir': pgm_record.get('pgmDir'),
                        'pgm_dir2': pgm_record.get('pgmDir2'),
                        'pgm_dir3': pgm_record.get('pgmDir3'),
                        'pgm_dir4': pgm_record.get('pgmDir4'),
                        'equipment_model_code': pgm_record.get('equipmentModelCode'),
                        'operation_id': pgm_record.get('operationId'),
                        'module_type': pgm_record.get('moduleType'),
                        'product_type': pgm_record.get('productType'),
                        'tech_nm': pgm_record.get('techNm'),
                        'pkg_den_typ': pgm_record.get('pkgDenTyp'),
                        'organiz_cd': pgm_record.get('organizCd'),
                        'den_typ': pgm_record.get('denTyp'),
                        'change_date_time': pgm_record.get('changeDateTime'),
                        'factory_id': pgm_record.get('factoryId')
                    }
                    extracted['pgm_records'].append(pgm_info)

            self.logger.info(f"ğŸ“Š æå–ETä¿¡æ¯: {len(extracted['pgm_records'])}æ¡PGMè®°å½•")
            return extracted

        except Exception as e:
            self.logger.error(f"âŒ æå–ETä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def _extract_at_info(self, data: Dict[str, Any], process_id: str, work_sequence: int) -> Dict[str, Any]:
        """æå–ATä¿¡æ¯"""
        try:
            extracted = {
                'process_id': process_id,
                'work_sequence': work_sequence,
                'type': 'AT',
                'extracted_at': datetime.now().isoformat(),
                'pgm_records': [],
                'file_info': [],
                'work_info': {}
            }

            # æå–å·¥ä½œè¯¦æƒ…
            if 'workDetailViews' in data and data['workDetailViews']:
                work_detail = data['workDetailViews'][0]
                extracted['work_info'] = {
                    'process_name': work_detail.get('processName'),
                    'work_name': work_detail.get('workName'),
                    'status': work_detail.get('status'),
                    'process_start_time': work_detail.get('processStartTime'),
                    'process_end_time': work_detail.get('processEndTime'),
                    'work_start_time': work_detail.get('workStartTime'),
                    'work_end_time': work_detail.get('workEndTime'),
                    'factory': work_detail.get('factory'),
                    'specified_work_user': work_detail.get('specifiedWorkUserName')
                }

                # æå–æ–‡ä»¶ä¿¡æ¯
                if 'file' in work_detail and work_detail['file']:
                    for file_item in work_detail['file']:
                        file_info = {
                            'file_download_id': file_item.get('fileDownloadId'),
                            'file_name': file_item.get('fileName'),
                            'size': file_item.get('size')
                        }
                        extracted['file_info'].append(file_info)

            # æå–PGMè®°å½•
            if 'testProgramModuleDramAtViews' in data:
                for pgm_record in data['testProgramModuleDramAtViews']:
                    pgm_info = {
                        'draft_id': pgm_record.get('draftId'),
                        'pgm_id': pgm_record.get('pgmId'),
                        'pgm_rev_ver': pgm_record.get('pgmRevVer'),
                        'pgm_dir': pgm_record.get('pgmDir'),
                        'hdiag_dir': pgm_record.get('hdiagDir'),
                        'test_board_id': pgm_record.get('testBoardId'),
                        'operation_id': pgm_record.get('operationId'),
                        'module_type': pgm_record.get('moduleType'),
                        'product_type': pgm_record.get('productType'),
                        'tech_nm': pgm_record.get('techNm'),
                        'pkg_den_typ': pgm_record.get('pkgDenTyp'),
                        'sap_history_code': pgm_record.get('sapHistoryCode'),
                        'temper_val': pgm_record.get('temperVal'),
                        'change_date_time': pgm_record.get('changeDateTime'),
                        'factory_id': pgm_record.get('factoryId')
                    }
                    extracted['pgm_records'].append(pgm_info)

            self.logger.info(f"ğŸ“Š æå–ATä¿¡æ¯: {len(extracted['pgm_records'])}æ¡PGMè®°å½•")
            return extracted

        except Exception as e:
            self.logger.error(f"âŒ æå–ATä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def get_pgm_details_with_files(self,
                                   process_id: str,
                                   work_sequence: int,
                                   pgm_type: str = None) -> Optional[Dict[str, Any]]:
        """
        è·å–PGMè¯¦æƒ…ï¼ˆåŒ…å«æ–‡ä»¶ä¿¡æ¯ï¼‰

        Args:
            process_id: æµç¨‹ID
            work_sequence: å·¥ä½œåºåˆ—
            pgm_type: PGMç±»å‹ï¼ˆET/ATï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ¤æ–­

        Returns:
            PGMè¯¦æƒ…ï¼ˆåŒ…å«æ–‡ä»¶ä¿¡æ¯ï¼‰
        """
        try:
            if not self._ensure_login():
                self.logger.error("âŒ è·å–PGMè¯¦æƒ…å¤±è´¥: ç™»å½•æ— æ•ˆ")
                return None

            # æ ¹æ®ç±»å‹é€‰æ‹©ç«¯ç‚¹
            if pgm_type == 'ET' or pgm_type == 'et':
                endpoint = 'et_detail'
            elif pgm_type == 'AT' or pgm_type == 'at':
                endpoint = 'at_detail'
            else:
                # é»˜è®¤ä½¿ç”¨ET
                endpoint = 'et_detail'
                self.logger.warning("âš ï¸ æœªæŒ‡å®šPGMç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨ET")

            url = urljoin(self.api_base, self.endpoints[endpoint])

            params = {
                "processId": process_id,
                "workSequence": work_sequence
            }

            self.logger.info(f"ğŸ“‹ è·å–PGMè¯¦æƒ…: {process_id}, ç±»å‹: {pgm_type or 'è‡ªåŠ¨'}")

            response = self.session.get(
                url,
                params=params,
                headers=self._get_headers(endpoint),
                timeout=self.timeouts['detail']
            )

            response.raise_for_status()
            data = response.json()

            # æå–æ–‡ä»¶ä¿¡æ¯
            file_list = []

            # ä»workDetailViewsä¸­æå–
            if 'workDetailViews' in data and data['workDetailViews']:
                work_detail = data['workDetailViews'][0]
                if 'file' in work_detail and work_detail['file']:
                    for file_item in work_detail['file']:
                        file_info = {
                            'file_download_id': file_item.get('fileDownloadId'),
                            'file_name': file_item.get('fileName'),
                            'size': file_item.get('size'),
                            'type': 'attachment'  # é™„ä»¶æ–‡ä»¶
                        }
                        file_list.append(file_info)

            # ä»PGMè®°å½•ä¸­æå–å¯èƒ½çš„æ–‡ä»¶ä¿¡æ¯
            pgm_records = []
            if 'testProgramModuleDramEtViews' in data:
                pgm_records.extend(data['testProgramModuleDramEtViews'])
            if 'testProgramModuleDramAtViews' in data:
                pgm_records.extend(data['testProgramModuleDramAtViews'])

            for pgm_record in pgm_records:
                # æå–å¯èƒ½çš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                pgm_dir = pgm_record.get('pgmDir')
                if pgm_dir:
                    file_info = {
                        'file_download_id': None,  # éœ€è¦å•ç‹¬è·å–
                        'file_name': os.path.basename(pgm_dir) if pgm_dir else None,
                        'pgm_dir': pgm_dir,
                        'type': 'pgm_file'
                    }
                    file_list.append(file_info)

            result = {
                'process_id': process_id,
                'work_sequence': work_sequence,
                'pgm_type': pgm_type,
                'files': file_list,
                'raw_data': data  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºè°ƒè¯•
            }

            self.logger.info(f"âœ… è·å–åˆ° {len(file_list)} ä¸ªæ–‡ä»¶ä¿¡æ¯")
            return result

        except Exception as e:
            self.logger.error(f"âŒ è·å–PGMè¯¦æƒ…å¤±è´¥: {str(e)}")
            return None

    def download_file(self,
                      file_download_id: str,
                      file_name: str,
                      process_id: str,
                      work_sequence: int,
                      save_dir: str = None,
                      custom_filename: str = None) -> Tuple[bool, str]:
        """
        ä¸‹è½½å•ä¸ªæ–‡ä»¶

        Args:
            file_download_id: æ–‡ä»¶ä¸‹è½½ID
            file_name: åŸå§‹æ–‡ä»¶å
            process_id: æµç¨‹ID
            work_sequence: å·¥ä½œåºåˆ—
            save_dir: ä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®è·¯å¾„ï¼‰
            custom_filename: è‡ªå®šä¹‰æ–‡ä»¶å

        Returns:
            (æ˜¯å¦æˆåŠŸ, ä¿å­˜çš„æ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            if not self._ensure_login():
                return False, "ä¸‹è½½æ–‡ä»¶å¤±è´¥: ç™»å½•æ— æ•ˆ"

            # ç¡®å®šä¿å­˜ç›®å½•
            if save_dir is None:
                save_dir = self.file_paths['local_verify']

            # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            save_path_obj = Path(save_dir)
            save_path_obj.mkdir(parents=True, exist_ok=True)

            # ç¡®å®šæ–‡ä»¶å
            if custom_filename:
                filename = custom_filename
            else:
                # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
                filename = self._sanitize_filename(file_name)

            # æ„å»ºå®Œæ•´ä¿å­˜è·¯å¾„
            save_path = save_path_obj / filename

            # æ„å»ºä¸‹è½½URLå’Œå‚æ•°
            download_url = urljoin(self.api_base, self.endpoints['file_download'])

            params = {
                "id": file_download_id,
                "userId": self.credentials.get('user_id', ''),
                "bpmsProcessId": process_id,
                "bpmsWorkSequence": work_sequence,
                "uiId": "ModuleTestPgmDistributeStatus"
            }

            self.logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ–‡ä»¶: {filename}")
            self.logger.debug(f"ä¸‹è½½URL: {download_url}")
            self.logger.debug(f"å‚æ•°: {params}")

            # å‘é€ä¸‹è½½è¯·æ±‚
            response = self.session.get(
                download_url,
                params=params,
                headers=self._get_headers('file_download'),
                stream=True,
                timeout=self.timeouts['download']
            )

            response.raise_for_status()

            # å°è¯•ä»Content-Dispositionè·å–æ–‡ä»¶å
            content_disposition = response.headers.get('content-disposition', '')
            if 'filename=' in content_disposition:
                match = re.search(r'filename="?([^"]+)"?', content_disposition)
                if match:
                    server_filename = match.group(1)
                    filename = self._sanitize_filename(server_filename)
                    save_path = save_path_obj / filename
                    self.logger.info(f"ğŸ“ ä½¿ç”¨æœåŠ¡å™¨æä¾›çš„æ–‡ä»¶å: {filename}")

            # è·å–æ–‡ä»¶å¤§å°
            total_size = int(response.headers.get('content-length', 0))
            content_type = response.headers.get('content-type', 'application/octet-stream')

            self.logger.info(f"ğŸ“„ æ–‡ä»¶ç±»å‹: {content_type}, å¤§å°: {self._format_size(total_size)}")

            # ä¸‹è½½æ–‡ä»¶
            downloaded = 0
            start_time = time.time()

            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)

                        # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            current_time = time.time()
                            elapsed = current_time - start_time

                            # æ¯ä¸‹è½½5MBæˆ–5ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                            if downloaded % (5 * 1024 * 1024) == 0 or elapsed > 5:
                                speed = downloaded / elapsed / 1024 if elapsed > 0 else 0
                                self.logger.info(
                                    f"ğŸ“¥ ä¸‹è½½è¿›åº¦: {percent:.1f}% ({self._format_size(downloaded)}/{self._format_size(total_size)}) "
                                    f"é€Ÿåº¦: {speed:.1f} KB/s"
                                )

            # éªŒè¯æ–‡ä»¶
            actual_size = os.path.getsize(save_path)
            download_time = time.time() - start_time

            if total_size > 0 and actual_size != total_size:
                self.logger.warning(
                    f"âš ï¸ æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›={self._format_size(total_size)}, å®é™…={self._format_size(actual_size)}")
            else:
                self.logger.info(f"âœ… æ–‡ä»¶å¤§å°éªŒè¯é€šè¿‡")

            # è®¡ç®—ä¸‹è½½é€Ÿåº¦
            speed = actual_size / download_time / 1024 if download_time > 0 else 0

            self.logger.info(f"âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ: {save_path}")
            self.logger.info(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡ - å¤§å°: {self._format_size(actual_size)}, "
                             f"æ—¶é—´: {download_time:.2f}ç§’, é€Ÿåº¦: {speed:.1f} KB/s")

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆå¯é€‰ï¼‰
            file_hash = self._calculate_file_hash(str(save_path))
            if file_hash:
                self.logger.debug(f"ğŸ”¢ æ–‡ä»¶MD5: {file_hash}")

            return True, str(save_path)

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else "æœªçŸ¥"
            error_msg = f"ä¸‹è½½å¤±è´¥ (HTTP {status_code}): {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            return False, error_msg
        except Exception as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            return False, error_msg

    def download_all_pgm_files(self,
                               pgm_record: Dict[str, Any],
                               target_dir: str = None) -> Dict[str, Any]:
        """
        ä¸‹è½½PGMæ‰€æœ‰ç›¸å…³æ–‡ä»¶

        Args:
            pgm_record: PGMè®°å½•ï¼ˆä»get_pgm_distribution_statusè·å–ï¼‰
            target_dir: ç›®æ ‡ç›®å½•ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®è·¯å¾„ï¼‰

        Returns:
            ä¸‹è½½ç»“æœæ±‡æ€»
        """
        results = {
            'success': False,
            'pgm_info': {},
            'downloaded_files': [],
            'failed_files': [],
            'total_size': 0,
            'total_time': 0,
            'save_directory': ''
        }

        try:
            process_id = pgm_record.get('processId')
            work_sequence = pgm_record.get('workSequence')

            if not process_id or not work_sequence:
                results['error'] = "ç¼ºå°‘process_idæˆ–work_sequence"
                return results

            # æå–PGMä¿¡æ¯
            pgm_info = self.get_file_info_from_pgm(pgm_record)
            results['pgm_info'] = pgm_info

            # ç¡®å®šä¿å­˜ç›®å½•
            if target_dir is None:
                # ä½¿ç”¨é…ç½®è·¯å¾„ + PGMç±»å‹ + draft_id
                base_dir = self.file_paths['local_verify']
                pgm_type = pgm_info.get('pgm_type', 'UNKNOWN')
                draft_id = pgm_info.get('draft_id', 'unknown')
                date_str = datetime.now().strftime("%Y%m%d")

                target_dir = os.path.join(base_dir, date_str, f"{pgm_type}_{draft_id}")

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(target_dir).mkdir(parents=True, exist_ok=True)
            results['save_directory'] = target_dir

            self.logger.info(f"ğŸ“‚ ä¿å­˜åˆ°ç›®å½•: {target_dir}")

            # è·å–PGMè¯¦æƒ…ï¼ˆåŒ…å«æ–‡ä»¶ä¿¡æ¯ï¼‰
            pgm_type = pgm_info.get('pgm_type')
            details = self.get_pgm_details_with_files(process_id, work_sequence, pgm_type)

            if not details or 'files' not in details:
                results['error'] = "è·å–PGMæ–‡ä»¶ä¿¡æ¯å¤±è´¥"
                return results

            # ä¸‹è½½æ‰€æœ‰æ–‡ä»¶
            start_time = time.time()
            downloaded_count = 0
            failed_count = 0

            for file_info in details['files']:
                file_download_id = file_info.get('file_download_id')
                file_name = file_info.get('file_name', f"file_{file_download_id or 'unknown'}")

                if not file_download_id:
                    self.logger.warning(f"âš ï¸ æ–‡ä»¶ {file_name} æ²¡æœ‰download_idï¼Œè·³è¿‡")
                    continue

                self.logger.info(f"â¬‡ï¸  ä¸‹è½½æ–‡ä»¶: {file_name}")

                success, file_path = self.download_file(
                    file_download_id=file_download_id,
                    file_name=file_name,
                    process_id=process_id,
                    work_sequence=work_sequence,
                    save_dir=target_dir
                )

                if success:
                    downloaded_count += 1
                    file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0

                    results['downloaded_files'].append({
                        'file_name': file_name,
                        'file_path': file_path,
                        'size': file_size,
                        'type': file_info.get('type', 'unknown')
                    })
                    results['total_size'] += file_size
                else:
                    failed_count += 1
                    results['failed_files'].append({
                        'file_name': file_name,
                        'error': file_path  # è¿™é‡Œfile_pathå®é™…ä¸Šæ˜¯é”™è¯¯ä¿¡æ¯
                    })

            # è®¡ç®—æ€»æ—¶é—´
            total_time = time.time() - start_time
            results['total_time'] = total_time

            # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
            if downloaded_count > 0:
                results['success'] = True

            self.logger.info(f"ğŸ“Š ä¸‹è½½å®Œæˆ - æˆåŠŸ: {downloaded_count}, å¤±è´¥: {failed_count}, "
                             f"æ€»å¤§å°: {self._format_size(results['total_size'])}, "
                             f"æ€»æ—¶é—´: {total_time:.2f}ç§’")

            # å¦‚æœä¸‹è½½äº†æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªæ‘˜è¦æ–‡ä»¶
            if results['downloaded_files']:
                self._create_download_summary(results, target_dir)

            return results

        except Exception as e:
            error_msg = f"ä¸‹è½½PGMæ–‡ä»¶å¤±è´¥: {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            results['error'] = error_msg
            return results

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        if not filename:
            return "unknown_file"

        # ç§»é™¤éæ³•å­—ç¬¦
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')

        # é™åˆ¶é•¿åº¦
        if len(filename) > 255:
            name, ext = os.path.splitext(filename)
            filename = name[:250 - len(ext)] + ext

        return filename

    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes == 0:
            return "0B"

        units = ['B', 'KB', 'MB', 'GB']
        i = 0
        size = float(size_bytes)

        while size >= 1024 and i < len(units) - 1:
            size /= 1024
            i += 1

        return f"{size:.2f} {units[i]}"

    def _calculate_file_hash(self, file_path: str, algorithm: str = 'md5') -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        if not os.path.exists(file_path):
            return ""

        hash_func = hashlib.new(algorithm)

        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b''):
                    hash_func.update(chunk)
            return hash_func.hexdigest()
        except:
            return ""

    def _create_download_summary(self, results: Dict[str, Any], target_dir: str):
        """åˆ›å»ºä¸‹è½½æ‘˜è¦æ–‡ä»¶"""
        try:
            summary_path = Path(target_dir) / "download_summary.json"

            summary = {
                'timestamp': datetime.now().isoformat(),
                'pgm_info': results.get('pgm_info', {}),
                'download_stats': {
                    'successful': len(results.get('downloaded_files', [])),
                    'failed': len(results.get('failed_files', [])),
                    'total_size': results.get('total_size', 0),
                    'total_time': results.get('total_time', 0)
                },
                'downloaded_files': results.get('downloaded_files', []),
                'failed_files': results.get('failed_files', [])
            }

            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)

            self.logger.info(f"ğŸ“ åˆ›å»ºä¸‹è½½æ‘˜è¦: {summary_path}")

        except Exception as e:
            self.logger.warning(f"âš ï¸ åˆ›å»ºä¸‹è½½æ‘˜è¦å¤±è´¥: {str(e)}")


class FileDownloadManager:
    """æ–‡ä»¶ä¸‹è½½ç®¡ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–ä¸‹è½½ç®¡ç†å™¨"""
        self.logger = get_pgm_logger().get_logger('download_manager')
        self.oms_client = OMSClient()
        self.config = get_config()

    def download_new_pgm_files(self, days: int = 1) -> List[Dict[str, Any]]:
        """
        ä¸‹è½½æœ€è¿‘å‡ å¤©çš„æ–°PGMæ–‡ä»¶

        Args:
            days: æ£€æŸ¥æœ€è¿‘å‡ å¤©çš„æ•°æ®

        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        all_results = []

        try:
            # 1. ç™»å½•
            self.logger.info(f"ğŸ” å¼€å§‹ä¸‹è½½æœ€è¿‘ {days} å¤©çš„æ–°PGMæ–‡ä»¶")

            success, message = self.oms_client.login()
            if not success:
                self.logger.error(f"âŒ ç™»å½•å¤±è´¥: {message}")
                return all_results

            # 2. è·å–PGMåˆ†å‘çŠ¶æ€
            today = datetime.now()
            begin_date = (today - timedelta(days=days)).strftime("%Y-%m-%d 00:00:00")
            end_date = today.strftime("%Y-%m-%d 23:59:59")

            self.logger.info(f"ğŸ“‹ è·å–PGMåˆ†å‘çŠ¶æ€: {begin_date} åˆ° {end_date}")

            # è¿™é‡Œéœ€è¦è°ƒç”¨åŸå§‹çš„OMSå®¢æˆ·ç«¯è·å–æ•°æ®
            # ç”±äºæ¨¡å—ä¾èµ–ï¼Œæˆ‘ä»¬å…ˆç®€å•å®ç°
            # å®é™…ä½¿ç”¨æ—¶åº”è¯¥ä½¿ç”¨åŸå§‹çš„OMSClient

            # 3. æ¨¡æ‹Ÿä¸‹è½½æµ‹è¯•æ–‡ä»¶
            self.logger.info("âš ï¸  æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿä¸‹è½½ï¼Œéœ€è¦çœŸå®çš„file_download_idæ‰èƒ½ä¸‹è½½å®é™…æ–‡ä»¶")

            # åˆ›å»ºæµ‹è¯•ç›®å½•ç»“æ„
            self._create_test_directory_structure()

            # ä¸‹è½½ç¤ºä¾‹æ–‡ä»¶
            test_result = self._download_example_file()
            if test_result:
                all_results.append(test_result)

            return all_results

        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è½½PGMæ–‡ä»¶å¤±è´¥: {str(e)}")
            return all_results

    def _create_test_directory_structure(self):
        """åˆ›å»ºæµ‹è¯•ç›®å½•ç»“æ„"""
        try:
            file_paths = self.config.get_file_paths()
            verify_dir = file_paths['local_verify']

            # åˆ›å»ºä¸»ç›®å½•
            Path(verify_dir).mkdir(parents=True, exist_ok=True)

            # åˆ›å»ºå­ç›®å½•ç¤ºä¾‹
            subdirs = ['ET', 'AT', 'HESS', 'ARCHIVE']
            for subdir in subdirs:
                Path(verify_dir).joinpath(subdir).mkdir(exist_ok=True)

            self.logger.info(f"ğŸ“ åˆ›å»ºç›®å½•ç»“æ„: {verify_dir}")

        except Exception as e:
            self.logger.warning(f"âš ï¸ åˆ›å»ºç›®å½•ç»“æ„å¤±è´¥: {str(e)}")

    def _download_example_file(self) -> Dict[str, Any]:
        """ä¸‹è½½ç¤ºä¾‹æ–‡ä»¶ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        try:
            # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
            test_dir = self.config.get_file_paths()['local_verify']
            test_file_path = Path(test_dir) / "test_example.txt"

            with open(test_file_path, 'w') as f:
                f.write("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œç”¨äºéªŒè¯æ–‡ä»¶ä¸‹è½½åŠŸèƒ½ã€‚\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\n")
                f.write("å®é™…æ–‡ä»¶ä¸‹è½½éœ€è¦æœ‰æ•ˆçš„file_download_idã€‚\n")

            file_size = os.path.getsize(test_file_path)

            result = {
                'success': True,
                'file_path': str(test_file_path),
                'size': file_size,
                'type': 'test',
                'message': 'ç¤ºä¾‹æ–‡ä»¶å·²åˆ›å»ºï¼Œå®é™…ä¸‹è½½éœ€è¦æœ‰æ•ˆçš„file_download_id'
            }

            self.logger.info(f"ğŸ“ åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {test_file_path}")
            return result

        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºç¤ºä¾‹æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None


def real_download_test(file_download_id: str, process_id: str, work_sequence: int):
    """çœŸå®æ–‡ä»¶ä¸‹è½½æµ‹è¯•ï¼ˆéœ€è¦æœ‰æ•ˆçš„å‚æ•°ï¼‰"""
    print("=" * 60)
    print("çœŸå®æ–‡ä»¶ä¸‹è½½æµ‹è¯•")
    print("=" * 60)

    from utils.logger import get_pgm_logger
    logger = get_pgm_logger()
    logger.log_execution_start("çœŸå®æ–‡ä»¶ä¸‹è½½æµ‹è¯•")

    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        oms_client = OMSClient()

        # ç™»å½•
        success, message = oms_client.login()
        if not success:
            print(f"âŒ ç™»å½•å¤±è´¥: {message}")
            return False

        print(f"âœ… ç™»å½•æˆåŠŸ")
        # è·å–PGM List
        pgm_list = oms_client.get_new_pgms()
        print(f"ğŸ“‹ è·å–åˆ°çš„PGMåˆ—è¡¨: {pgm_list}")
        # ä¸‹è½½æ–‡ä»¶
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ–‡ä»¶...")
        print(f"   æ–‡ä»¶ID: {file_download_id}")
        print(f"   æµç¨‹ID: {process_id}")
        print(f"   å·¥ä½œåºåˆ—: {work_sequence}")

        success, result = oms_client.download_file(
            file_download_id=file_download_id,
            file_name=f"downloaded_file_{file_download_id[:8]}",
            process_id=process_id,
            work_sequence=work_sequence
        )

        if success:
            print(f"âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {result}")

            # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
            if os.path.exists(result):
                file_size = os.path.getsize(result)
                print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {oms_client._format_size(file_size)}")

                # æ£€æŸ¥æ–‡ä»¶ç±»å‹
                import mimetypes
                file_type, _ = mimetypes.guess_type(result)
                print(f"ğŸ“‹ æ–‡ä»¶ç±»å‹: {file_type or 'æœªçŸ¥'}")
        else:
            print(f"âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥: {result}")

        logger.log_execution_end("çœŸå®æ–‡ä»¶ä¸‹è½½æµ‹è¯•", success=success)
        return success

    except Exception as e:
        logger.log_execution_end("çœŸå®æ–‡ä»¶ä¸‹è½½æµ‹è¯•", success=False, message=str(e))
        print(f"âŒ çœŸå®ä¸‹è½½æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


class OMSDataProcessor:
    """OMSæ•°æ®å¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆæ•°æ®å¤„ç†å™¨"""
        super().__init__()
        self.oms_client = OMSClient()  # ä½¿ç”¨ä¼˜åŒ–åçš„å®¢æˆ·ç«¯

    def download_pgm_files(self,
                           pgm_data: Dict[str, Any],
                           save_base_dir: str = None) -> Dict[str, Any]:
        """
        ä¸‹è½½PGMç›¸å…³æ–‡ä»¶ï¼ˆHESSæ–‡ä»¶å’ŒPGMæ–‡ä»¶ï¼‰

        Args:
            pgm_data: PGMæ•°æ®ï¼ŒåŒ…å«process_idå’Œwork_sequence
            save_base_dir: ä¿å­˜åŸºç¡€ç›®å½•

        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        if not save_base_dir:
            config = get_config()
            save_base_dir = config.get_file_paths()['local_verify']

        results = {
            'hess_downloaded': False,
            'pgm_downloaded': False,
            'hess_path': None,
            'pgm_path': None,
            'errors': []
        }

        try:
            process_id = pgm_data.get('process_id')
            work_sequence = pgm_data.get('work_sequence')

            if not process_id or not work_sequence:
                results['errors'].append("ç¼ºå°‘process_idæˆ–work_sequence")
                return results

            # 1. å…ˆè·å–PGMè¯¦æƒ…ï¼ˆåŒ…å«æ–‡ä»¶ä¿¡æ¯ï¼‰
            pgm_type = 'et' if pgm_data.get('type') == 'ET' else 'at'

            if pgm_type == 'et':
                details = self.oms_client.get_et_pgm_details(process_id, work_sequence)
            else:
                details = self.oms_client.get_at_pgm_details(process_id, work_sequence)

            if not details:
                results['errors'].append("è·å–PGMè¯¦æƒ…å¤±è´¥")
                return results

            # 2. åˆ›å»ºä¿å­˜ç›®å½•
            save_dir = Path(save_base_dir) / f"{pgm_type.upper()}_{process_id}"
            save_dir.mkdir(parents=True, exist_ok=True)

            # 3. ä¸‹è½½æ–‡ä»¶
            file_infos = []

            # HESSæ–‡ä»¶
            for file_info in details.get('file_info', []):
                if 'hess' in file_info.get('fileName', '').lower() or '.xlsx' in file_info.get('fileName', ''):
                    file_infos.append({
                        **file_info,
                        'bpmsProcessId': process_id,
                        'bpmsWorkSequence': work_sequence
                    })

            # PGMæ–‡ä»¶ï¼ˆä»pgm_recordsä¸­è·å–ï¼‰
            for pgm_record in details.get('pgm_records', []):
                # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æå–PGMæ–‡ä»¶ä¿¡æ¯
                # å®é™…å®ç°éœ€è¦æ ¹æ®APIè¿”å›çš„å…·ä½“ç»“æ„è°ƒæ•´
                pass

            # æ‰¹é‡ä¸‹è½½
            if file_infos:
                download_results = self.oms_client.batch_download_files(
                    file_infos, str(save_dir), max_concurrent=2
                )

                for result in download_results:
                    if result['success']:
                        if 'hess' in result['file_info'].get('fileName', '').lower():
                            results['hess_downloaded'] = True
                            results['hess_path'] = result['file_path']
                        else:
                            results['pgm_downloaded'] = True
                            results['pgm_path'] = result['file_path']
                    else:
                        results['errors'].append(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {result['error']}")

            return results

        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è½½PGMæ–‡ä»¶å¤±è´¥: {str(e)}")
            results['errors'].append(str(e))
            return results


if __name__ == "__main__":

    print("\n" + "=" * 60)
    real_download_test(
        file_download_id="7186051",
        process_id="85c7d1b3-aabb-4aef-88e1-b3a0e3de6d19",
        work_sequence=1
    )
