"""
OMSå®¢æˆ·ç«¯æ¨¡å— - è´Ÿè´£ä¸SK Hynix OMSç³»ç»Ÿäº¤äº’
"""
import requests
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from urllib.parse import urljoin
import time
import hashlib
import sys
import os
from pathlib import Path
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))  # coreç›®å½•
dev_dir = os.path.dirname(current_dir)  # devç›®å½•
project_root = os.path.dirname(dev_dir)  # é¡¹ç›®æ ¹ç›®å½•

sys.path.insert(0, dev_dir)
sys.path.insert(0, project_root)
from utils.config_loader import get_config
from utils.logger import get_pgm_logger


class OMSClient:
    """OMSç³»ç»Ÿå®¢æˆ·ç«¯"""

    def __init__(self):
        """åˆå§‹åŒ–OMSå®¢æˆ·ç«¯"""
        self.logger = get_pgm_logger().get_logger('oms')
        self.config = get_config().get_oms_config()

        # è·å–ç«¯ç‚¹é…ç½®
        self.endpoints = self.config['endpoints']
        self.api_base = self.config['api_base']

        # è®¤è¯ä¿¡æ¯
        config_loader = get_config()
        env = config_loader.get_current_environment()
        if env in self.config.get('credentials', {}):
            self.credentials = self.config['credentials'][env]
        else:
            # å›é€€åˆ°é»˜è®¤å‡­æ®ç»“æ„ï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
            self.credentials = self.config.get('credentials', {})

        # Tokenç®¡ç†
        self.token = None
        self.token_expiry = None
        self.token_refresh_buffer = timedelta(minutes=5)  # æå‰5åˆ†é’Ÿåˆ·æ–°token

        # è¯·æ±‚è¶…æ—¶é…ç½®
        self.timeouts = self.config.get('request_timeout', {
            'login': 30,
            'list': 60,
            'detail': 60,
            'download': 300
        })

        # é‡è¯•é…ç½®
        self.retry_settings = self.config.get('retry_settings', {
            'max_retries': 3,
            'retry_delay_seconds': 5,
            'retry_on_status_codes': [408, 429, 500, 502, 503, 504]
        })

        # ç¼“å­˜é…ç½®
        self.cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

        # åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session
        self.session = self._create_retry_session()

        self.logger.info(f"ğŸ”§ OMSå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œç¯å¢ƒ: {env}")

        self.file_paths = get_config().get_file_paths()

    def _create_retry_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session"""
        session = requests.Session()

        retry_strategy = Retry(
            total=self.retry_settings['max_retries'],
            backoff_factor=self.retry_settings['retry_delay_seconds'],
            status_forcelist=self.retry_settings['retry_on_status_codes']
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)

        return session

    def _get_headers(self, endpoint_name: str = None) -> Dict[str, str]:
        """
        è·å–è¯·æ±‚å¤´

        Args:
            endpoint_name: ç«¯ç‚¹åç§°ï¼Œç”¨äºè·å–ç‰¹å®šç«¯ç‚¹çš„headers

        Returns:
            è¯·æ±‚å¤´å­—å…¸
        """
        headers = self.config['headers']['common'].copy()

        # æ·»åŠ ç‰¹å®šç«¯ç‚¹çš„headers
        if endpoint_name and 'endpoint_specific' in self.config['headers']:
            specific_headers = self.config['headers']['endpoint_specific'].get(endpoint_name, {})
            headers.update(specific_headers)

        # æ·»åŠ Authorizationå¤´
        if self.token and self._is_token_valid():
            headers['Authorization'] = self.token

        return headers

    def login(self, force: bool = False) -> Tuple[bool, str]:
        """
        ç™»å½•OMSç³»ç»Ÿè·å–token

        Args:
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç™»å½•

        Returns:
            (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æ£€æŸ¥tokenæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            if not force and self._is_token_valid():
                self.logger.debug("âœ… ä½¿ç”¨ç¼“å­˜çš„OMS token")
                return True, "ä½¿ç”¨ç¼“å­˜çš„token"

            login_url = urljoin(self.api_base, self.endpoints['auth'])

            # å‡†å¤‡ç™»å½•æ•°æ®
            login_data = {
                "id": self.credentials['user_id'],
                "password": self.credentials['password']
            }

            self.logger.info(f"ğŸ”‘ æ­£åœ¨ç™»å½•OMSç³»ç»Ÿ: {self.credentials['user_id']}")
            self.logger.debug(f"ç™»å½•URL: {login_url}")

            # å‘é€ç™»å½•è¯·æ±‚
            response = self.session.post(
                login_url,
                json=login_data,
                headers=self._get_headers(),
                timeout=self.timeouts['login']
            )

            response.raise_for_status()

            result = response.json()

            # æ£€æŸ¥å“åº”ç»“æ„
            if not isinstance(result, dict):
                return False, "ç™»å½•å“åº”æ ¼å¼é”™è¯¯"

            token = result.get("token")
            if not token:
                error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥: {error_msg}")
                return False, f"å“åº”ä¸­æœªæ‰¾åˆ°token: {error_msg}"

            # æ›´æ–°token
            self.token = f"Bearer {token}"

            # è§£ætokenè¿‡æœŸæ—¶é—´ï¼ˆä»å“åº”ä¸­è·å–æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            token_duration = result.get("expires_in", 3600)  # é»˜è®¤1å°æ—¶
            self.token_expiry = datetime.now() + timedelta(seconds=token_duration)

            self.logger.info(f"âœ… OMSç™»å½•æˆåŠŸï¼Œtokenæœ‰æ•ˆæœŸè‡³: {self.token_expiry}")
            return True, "ç™»å½•æˆåŠŸ"

        except requests.exceptions.Timeout:
            self.logger.error("âŒ OMSç™»å½•å¤±è´¥: è¯·æ±‚è¶…æ—¶")
            return False, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            self.logger.error("âŒ OMSç™»å½•å¤±è´¥: è¿æ¥é”™è¯¯")
            return False, "è¿æ¥é”™è¯¯"
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else "æœªçŸ¥"
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (HTTP {status_code}): {str(e)}")
            return False, f"HTTPé”™è¯¯: {status_code}"
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return False, "JSONè§£æé”™è¯¯"
        except Exception as e:
            self.logger.error(f"âŒ OMSç™»å½•å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return False, f"æœªçŸ¥é”™è¯¯: {str(e)}"

    def _is_token_valid(self) -> bool:
        """æ£€æŸ¥tokenæ˜¯å¦æœ‰æ•ˆ"""
        if not self.token or not self.token_expiry:
            return False

        # è€ƒè™‘åˆ·æ–°ç¼“å†²æ—¶é—´
        return datetime.now() < (self.token_expiry - self.token_refresh_buffer)

    def _ensure_login(self) -> bool:
        """ç¡®ä¿å·²ç™»å½•ï¼Œè‡ªåŠ¨å¤„ç†tokenåˆ·æ–°"""
        if not self._is_token_valid():
            success, message = self.login()
            if not success:
                self.logger.error(f"âŒ è‡ªåŠ¨ç™»å½•å¤±è´¥: {message}")
                return False
        return True

    # è·å–æ–°çš„PGM Listï¼Œå¦‚æœbegin_dateï¼Œend_dateä¸ºç©ºåˆ™ä¸º11å¤©å‰è‡³å½“å‰æ—¶é—´
    def get_pgm_distribution_status(self,
                                    begin_date: Optional[str] = None,
                                    end_date: Optional[str] = None,
                                    factory_id: str = "OSMOD",
                                    company_id: str = "HITECH",
                                    force_refresh: bool = False) -> List[Dict[str, Any]]:
        """
        è·å–PGMåˆ†å‘çŠ¶æ€

        Args:
            begin_date: å¼€å§‹æ—¥æœŸ (æ ¼å¼: "YYYY-MM-DD HH:MM:SS")
            end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: "YYYY-MM-DD HH:MM:SS")
            factory_id: å·¥å‚ID
            company_id: å…¬å¸ID
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜

        Returns:
            PGMåˆ†å‘çŠ¶æ€åˆ—è¡¨
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.md5(
            f"distribution_{factory_id}_{company_id}_{begin_date}_{end_date}".encode()
        ).hexdigest()

        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and cache_key in self.cache:
            cache_data, cache_time = self.cache[cache_key]
            if time.time() - cache_time < self.cache_ttl:
                self.logger.debug(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„PGMåˆ†å‘çŠ¶æ€æ•°æ®")
                return cache_data

        try:
            # ç¡®ä¿å·²ç™»å½•
            if not self._ensure_login():
                self.logger.error("âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥: ç™»å½•æ— æ•ˆ")
                return []

            # è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´
            today = datetime.now()
            if not begin_date:
                begin_date = (today - timedelta(days=11)).strftime("%Y-%m-%d 07:00:00")
            if not end_date:
                end_date = (today + timedelta(days=1)).strftime("%Y-%m-%d 07:00:00")
            self.logger.info(f"ğŸ“… æ—¥æœŸå‚æ•°: beginDate={begin_date}, endDate={end_date}")
            # æ„å»ºURLå’Œå‚æ•°
            url = urljoin(self.api_base, self.endpoints['distribute_status'])

            params = {
                "factoryId": factory_id,
                "companyId": company_id,
                "beginDate": begin_date,
                "endDate": end_date
            }

            self.logger.info(f"ğŸ“‹ è·å–PGMåˆ†å‘çŠ¶æ€: {begin_date} åˆ° {end_date}")
            self.logger.debug(f"è¯·æ±‚URL: {url}")
            self.logger.debug(f"è¯·æ±‚å‚æ•°: {params}")

            # å‘é€è¯·æ±‚
            start_time = time.time()
            response = self.session.get(
                url,
                params=params,
                headers=self._get_headers('distribute_status'),
                timeout=self.timeouts['list']
            )
            response_time = time.time() - start_time

            response.raise_for_status()

            data = response.json()

            if not isinstance(data, list):
                self.logger.error(f"âŒ å“åº”æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨ï¼Œå¾—åˆ°: {type(data)}")
                return []

            self.logger.info(f"âœ… æˆåŠŸè·å–PGMåˆ†å‘çŠ¶æ€ï¼Œå…±{len(data)}æ¡è®°å½•")
            self.logger.debug(f"å“åº”æ—¶é—´: {response_time:.2f}ç§’")

            # ç¼“å­˜ç»“æœ
            self.cache[cache_key] = (data, time.time())

            return data

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else "æœªçŸ¥"

            if status_code in [401, 403]:
                self.logger.info("ğŸ”„ Tokenè¿‡æœŸæˆ–æ— æ•ˆï¼Œå°è¯•é‡æ–°ç™»å½•...")
                self.token = None
                self.token_expiry = None

                # é‡æ–°ç™»å½•å¹¶é‡è¯•
                if self._ensure_login():
                    return self.get_pgm_distribution_status(
                        begin_date, end_date, factory_id, company_id, True
                    )

            self.logger.error(f"âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥ (HTTP {status_code}): {str(e)}")
            return []
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥ (ç½‘ç»œé”™è¯¯): {str(e)}")
            return []
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return []
        except Exception as e:
            self.logger.error(f"âŒ è·å–PGMåˆ†å‘çŠ¶æ€å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return []

    # æ ¹æ®pgm_record.processNameç¡®å®šæ˜¯ETè¿˜æ˜¯AT PGM
    def _determine_pgm_type(self, pgm_record: Dict[str, Any]) -> str:
        """ç¡®å®šPGMç±»å‹"""
        process_type = str(pgm_record.get('processType', '')).upper()

        if 'ET' in process_type:
            return 'ET'
        elif 'AT' in process_type:
            return 'AT'
        elif 'HESS' in process_type:
            return 'HESS'
        else:
            return 'UNKNOWN'

    # æ ¹æ®PGMç±»å‹è·å–HESSè¯¦æƒ…
    def _get_pgm_detial(self,
                        pgm_type: str,
                        process_id: str,
                        work_sequence: int) -> Optional[Dict[str, Any]]:
        """
        é€šç”¨æ–¹æ³•è·å–PGMè¯¦æƒ…

        Args:
            pgm_type: 'et' æˆ– 'at'
            process_id: æµç¨‹ID
            work_sequence: å·¥ä½œåºåˆ—å·

        Returns:
            PGMè¯¦ç»†ä¿¡æ¯
        """
        try:
            if not self._ensure_login():
                self.logger.error(f"âŒ è·å–{pgm_type.upper()}è¯¦æƒ…å¤±è´¥: ç™»å½•æ— æ•ˆ")
                return None

            # æ„å»ºURL
            endpoint_key = f"{pgm_type.lower()}_hess"
            url = urljoin(self.api_base, self.endpoints[endpoint_key])

            params = {
                "processId": process_id,
                "workSequence": work_sequence
            }

            self.logger.info(
                f"ğŸ“‹ è·å–{pgm_type.upper()} PGMè¯¦æƒ…: process_id={process_id}, work_sequence={work_sequence}")

            # å‘é€è¯·æ±‚
            start_time = time.time()
            response = self.session.get(
                url,
                params=params,
                headers=self._get_headers(endpoint_key),
                timeout=self.timeouts['detail']
            )
            response_time = time.time() - start_time

            response.raise_for_status()

            data = response.json()

            self.logger.info(f"âœ… æˆåŠŸè·å–{pgm_type.upper()} PGMè¯¦æƒ…ï¼Œå“åº”æ—¶é—´: {response_time:.2f}ç§’")

            # æå–å…³é”®ä¿¡æ¯
            if pgm_type == 'ET':
                extracted_data = self._extract_et_hess(data, process_id, work_sequence)
            else:
                extracted_data = self._extract_at_hess(data, process_id, work_sequence)

            return extracted_data

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else "æœªçŸ¥"
            self.logger.error(f"âŒ è·å–{pgm_type.upper()} PGMè¯¦æƒ…å¤±è´¥ (HTTP {status_code}): {str(e)}")
            return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ è·å–{pgm_type.upper()} PGMè¯¦æƒ…å¤±è´¥ (ç½‘ç»œé”™è¯¯): {str(e)}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ è·å–{pgm_type.upper()} PGMè¯¦æƒ…å¤±è´¥ (JSONè§£æé”™è¯¯): {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ è·å–{pgm_type.upper()} PGMè¯¦æƒ…å¤±è´¥ (æœªçŸ¥é”™è¯¯): {str(e)}")
            return None

    # ä»¥ETæ¨¡å¼æŠ½å–HESSä¿¡æ¯ï¼ˆä¸æ¶‰åŠOMSæ“ä½œï¼‰
    def _extract_et_hess(self, data: Dict[str, Any], process_id: str, work_sequence: int) -> Dict[str, Any]:
        """æå–ETä¿¡æ¯"""
        try:
            extracted = {
                'process_id': process_id,
                'work_sequence': work_sequence,
                'type': 'ET',
                'extracted_at': datetime.now().isoformat(),
                'pgm_records': [],
                'file_info': [],
                'work_info': {}
            }

            # æå–å·¥ä½œè¯¦æƒ…
            if 'workDetailViews' in data and data['workDetailViews']:
                work_detail = data['workDetailViews'][0]
                extracted['work_info'] = {
                    'process_name': work_detail.get('processName'),
                    'work_name': work_detail.get('workName'),
                    'status': work_detail.get('status'),
                    'process_start_time': work_detail.get('processStartTime'),
                    'process_end_time': work_detail.get('processEndTime'),
                    'work_start_time': work_detail.get('workStartTime'),
                    'work_end_time': work_detail.get('workEndTime'),
                    'factory': work_detail.get('factory'),
                    'specified_work_user': work_detail.get('specifiedWorkUserName')
                }

                # æå–æ–‡ä»¶ä¿¡æ¯
                if 'file' in work_detail and work_detail['file']:
                    for file_item in work_detail['file']:
                        file_info = {
                            'file_download_id': file_item.get('fileDownloadId'),
                            'file_name': file_item.get('fileName'),
                            'size': file_item.get('size')
                        }
                        extracted['file_info'].append(file_info)

            # æå–PGMè®°å½•
            if 'testProgramModuleDramEtViews' in data:
                for pgm_record in data['testProgramModuleDramEtViews']:
                    pgm_info = {
                        'draft_id': pgm_record.get('draftId'),
                        'pgm_id': pgm_record.get('pgmId'),
                        'pgm_rev_ver': pgm_record.get('pgmRevVer'),
                        'pgm_dir': pgm_record.get('pgmDir'),
                        'pgm_dir2': pgm_record.get('pgmDir2'),
                        'pgm_dir3': pgm_record.get('pgmDir3'),
                        'pgm_dir4': pgm_record.get('pgmDir4'),
                        'pgm_dir5': pgm_record.get('pgmDir5'),
                        'operation_id': pgm_record.get('operationId'),
                        'module_type': pgm_record.get('moduleType'),
                        'product_type': pgm_record.get('productType'),
                        'tech_nm': pgm_record.get('techNm'),
                        'pkg_den_typ': pgm_record.get('pkgDenTyp'),
                        'den_typ': pgm_record.get('denTyp'),
                        'change_date_time': pgm_record.get('changeDateTime'),
                        'factory_id': pgm_record.get('factoryId'),
                        'del_yn': pgm_record.get('delYn'),
                        'organiz_cd': pgm_record.get('organizCd'),
                        'special_cd': pgm_record.get('specialCd'),
                        'timekey': pgm_record.get('timekey'),
                        'controller_name_val': pgm_record.get('controllerNameVal'),
                        'fab_id': pgm_record.get('fabId'),
                        'fmw_ver_val': pgm_record.get('fmwVerVal'),
                        'grade_code': pgm_record.get('gradeCode'),
                        'mask_cd1': pgm_record.get('maskCd1'),
                        'mod_section_typ': pgm_record.get('modSectionTyp'),
                        'pkg_typ2': pgm_record.get('pkgTyp2'),
                        'qual_opt_cd2': pgm_record.get('qualOptCd2'),
                        'sap_history_code': pgm_record.get('sapHistoryCode'),
                        'tranmit_bp_list': pgm_record.get('tranmitBpList'),
                        'tsv_die_typ': pgm_record.get('tsvDieTyp'),
                        'ver_typ': pgm_record.get('verTyp'),
                        'drafted': pgm_record.get('drafted'),
                        'equipment_model_code': pgm_record.get('equipmentModelCode'),
                        'module_height_value': pgm_record.get('moduleHeightValue'),
                        'owner_code': pgm_record.get('ownerCode')
                    }
                    extracted['pgm_records'].append(pgm_info)

            self.logger.info(f"ğŸ“Š æå–ETä¿¡æ¯: {len(extracted['pgm_records'])}æ¡PGMè®°å½•")
            return extracted

        except Exception as e:
            self.logger.error(f"âŒ æå–ETä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    # ä»¥ATæ¨¡å¼æŠ½å–HESSä¿¡æ¯ï¼ˆä¸æ¶‰åŠOMSæ“ä½œï¼‰
    def _extract_at_hess(self, data: Dict[str, Any], process_id: str, work_sequence: int) -> Dict[str, Any]:
        """æå–ATä¿¡æ¯"""
        try:
            extracted = {
                'process_id': process_id,
                'work_sequence': work_sequence,
                'type': 'AT',
                'extracted_at': datetime.now().isoformat(),
                'pgm_records': [],
                'file_info': [],
                'work_info': {}
            }

            # æå–å·¥ä½œè¯¦æƒ…
            if 'workDetailViews' in data and data['workDetailViews']:
                work_detail = data['workDetailViews'][0]
                extracted['work_info'] = {
                    'process_name': work_detail.get('processName'),
                    'work_name': work_detail.get('workName'),
                    'status': work_detail.get('status'),
                    'process_start_time': work_detail.get('processStartTime'),
                    'process_end_time': work_detail.get('processEndTime'),
                    'work_start_time': work_detail.get('workStartTime'),
                    'work_end_time': work_detail.get('workEndTime'),
                    'factory': work_detail.get('factory'),
                    'specified_work_user': work_detail.get('specifiedWorkUserName')
                }

                # æå–æ–‡ä»¶ä¿¡æ¯
                if 'file' in work_detail and work_detail['file']:
                    for file_item in work_detail['file']:
                        file_info = {
                            'file_download_id': file_item.get('fileDownloadId'),
                            'file_name': file_item.get('fileName'),
                            'size': file_item.get('size')
                        }
                        extracted['file_info'].append(file_info)

            # æå–PGMè®°å½•
            if 'testProgramModuleDramAtViews' in data:
                for pgm_record in data['testProgramModuleDramAtViews']:
                    pgm_info = {
                        'draft_id': pgm_record.get('draftId'),
                        'draft_seq': pgm_record.get('draftSeq'),
                        'drafted': pgm_record.get('drafted'),
                        'pgm_id': pgm_record.get('pgmId'),
                        'pgm_rev_ver': pgm_record.get('pgmRevVer'),
                        'pgm_dir': pgm_record.get('pgmDir'),
                        'hdiag_dir': pgm_record.get('hdiagDir'),
                        'test_board_id': pgm_record.get('testBoardId'),
                        'operation_id': pgm_record.get('operationId'),
                        'module_type': pgm_record.get('moduleType'),
                        'product_type': pgm_record.get('productType'),
                        'tech_nm': pgm_record.get('techNm'),
                        'pkg_den_typ': pgm_record.get('pkgDenTyp'),
                        'sap_history_code': pgm_record.get('sapHistoryCode'),
                        'temper_val': pgm_record.get('temperVal'),
                        'change_date_time': pgm_record.get('changeDateTime'),
                        'factory_id': pgm_record.get('factoryId'),
                        'del_yn': pgm_record.get('delYn'),
                        'organiz_cd': pgm_record.get('organizCd'),
                        'qual_opt_cd2': pgm_record.get('qualOptCd2'),
                        'special_cd': pgm_record.get('specialCd'),
                        'timekey': pgm_record.get('timekey'),
                        'controller_name_val': pgm_record.get('controllerNameVal'),
                        'den_typ': pgm_record.get('denTyp'),
                        'fab_id': pgm_record.get('fabId'),
                        'fmw_ver_val': pgm_record.get('fmwVerVal'),
                        'grade_code': pgm_record.get('gradeCode'),
                        'mask_cd1': pgm_record.get('maskCd1'),
                        'mod_section_typ': pgm_record.get('modSectionTyp'),
                        'pkg_typ2': pgm_record.get('pkgTyp2'),
                        'product_special_handle_value': pgm_record.get('productSpecialHandleValue'),
                        'tranmit_bp_list': pgm_record.get('tranmitBpList'),
                        'tsv_die_typ': pgm_record.get('tsvDieTyp'),
                        'ver_typ': pgm_record.get('verTyp')
                    }
                    extracted['pgm_records'].append(pgm_info)

            self.logger.info(f"ğŸ“Š æå–ATä¿¡æ¯: {len(extracted['pgm_records'])}æ¡PGMè®°å½•")
            return extracted

        except Exception as e:
            self.logger.error(f"âŒ æå–ATä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    # ä¸‹è½½å•ä¸ªæ–‡ä»¶
    def _download_sigle_file(self,
                             file_download_id: str,
                             file_name: str,
                             process_id: str,
                             work_sequence: int,
                             save_dir: str = None,
                             custom_filename: str = None) -> Tuple[bool, str]:
        """
        ä¸‹è½½å•ä¸ªæ–‡ä»¶

        Args:
            file_download_id: æ–‡ä»¶ä¸‹è½½ID
            file_name: åŸå§‹æ–‡ä»¶å
            process_id: æµç¨‹ID
            work_sequence: å·¥ä½œåºåˆ—
            save_dir: ä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®è·¯å¾„ï¼‰
            custom_filename: è‡ªå®šä¹‰æ–‡ä»¶å

        Returns:
            (æ˜¯å¦æˆåŠŸ, ä¿å­˜çš„æ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            if not self._ensure_login():
                return False, "ä¸‹è½½æ–‡ä»¶å¤±è´¥: ç™»å½•æ— æ•ˆ"

            # ç¡®å®šä¿å­˜ç›®å½•
            if save_dir is None:
                save_dir = self.file_paths['local_verify']

            # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            save_path_obj = Path(save_dir)
            save_path_obj.mkdir(parents=True, exist_ok=True)

            # ç¡®å®šæ–‡ä»¶å
            if custom_filename:
                filename = custom_filename
            else:
                # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
                filename = self._sanitize_filename(file_name)


            # æ„å»ºå®Œæ•´ä¿å­˜è·¯å¾„
            save_path = save_path_obj / filename

            # æ„å»ºä¸‹è½½URLå’Œå‚æ•°
            download_url = urljoin(self.api_base, self.endpoints['file_download'])

            params = {
                "id": file_download_id,
                "userId": self.credentials.get('user_id', ''),
                "bpmsProcessId": process_id,
                "bpmsWorkSequence": work_sequence,
                "uiId": "ModuleTestPgmDistributeStatus"
            }

            self.logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ–‡ä»¶: {filename}")
            self.logger.debug(f"ä¸‹è½½URL: {download_url}")
            self.logger.debug(f"å‚æ•°: {params}")

            # å‘é€ä¸‹è½½è¯·æ±‚
            response = self.session.get(
                download_url,
                params=params,
                headers=self._get_headers('file_download'),
                stream=True,
                timeout=self.timeouts['download']
            )

            response.raise_for_status()

            # å°è¯•ä»Content-Dispositionè·å–æ–‡ä»¶å
            content_disposition = response.headers.get('content-disposition', '')
            if 'filename=' in content_disposition:
                match = re.search(r'filename="?([^"]+)"?', content_disposition)
                if match:
                    server_filename = match.group(1)
                    
                    # ä¿®å¤å¯èƒ½çš„ç¼–ç é—®é¢˜ï¼šæœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶åå¯èƒ½æ˜¯UTF-8è¢«é”™è¯¯è§£é‡Šä¸ºLatin-1çš„æƒ…å†µ
                    # ä¾‹å¦‚ï¼šéŸ©æ–‡ "ì ìš©.zip" å¯èƒ½å˜æˆ "Ã¬\xa0\x81Ã¬\x9a.zip"
                    try:
                        # å°†å¯èƒ½è¢«é”™è¯¯è§£é‡Šçš„Latin-1å­—ç¬¦ä¸²è½¬æ¢å›æ­£ç¡®çš„UTF-8
                        server_filename_bytes = server_filename.encode('latin-1')
                        server_filename = server_filename_bytes.decode('utf-8')
                    except (UnicodeEncodeError, UnicodeDecodeError):
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸæ ·
                        pass
                    
                    filename = self._sanitize_filename(server_filename)
                    save_path = save_path_obj / filename
                    self.logger.info(f"ğŸ“ ä½¿ç”¨æœåŠ¡å™¨æä¾›çš„æ–‡ä»¶å: {filename}")
            # è·å–æ–‡ä»¶å¤§å°
            total_size = int(response.headers.get('content-length', 0))
            content_type = response.headers.get('content-type', 'application/octet-stream')

            self.logger.info(f"ğŸ“„ æ–‡ä»¶ç±»å‹: {content_type}, å¤§å°: {self._format_size(total_size)}")

            # ä¸‹è½½æ–‡ä»¶
            downloaded = 0
            start_time = time.time()

            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)

                        # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            current_time = time.time()
                            elapsed = current_time - start_time

                            # æ¯ä¸‹è½½5MBæˆ–5ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                            if downloaded % (5 * 1024 * 1024) == 0 or elapsed > 5:
                                speed = downloaded / elapsed / 1024 if elapsed > 0 else 0
                                self.logger.info(
                                    f"ğŸ“¥ ä¸‹è½½è¿›åº¦: {percent:.1f}% ({self._format_size(downloaded)}/{self._format_size(total_size)}) "
                                    f"é€Ÿåº¦: {speed:.1f} KB/s"
                                )

            # éªŒè¯æ–‡ä»¶
            actual_size = os.path.getsize(save_path)
            download_time = time.time() - start_time

            if total_size > 0 and actual_size != total_size:
                self.logger.warning(
                    f"âš ï¸ æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›={self._format_size(total_size)}, å®é™…={self._format_size(actual_size)}")
            else:
                self.logger.info(f"âœ… æ–‡ä»¶å¤§å°éªŒè¯é€šè¿‡")

            # è®¡ç®—ä¸‹è½½é€Ÿåº¦
            speed = actual_size / download_time / 1024 if download_time > 0 else 0

            self.logger.info(f"âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ: {save_path}")
            self.logger.info(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡ - å¤§å°: {self._format_size(actual_size)}, "
                             f"æ—¶é—´: {download_time:.2f}ç§’, é€Ÿåº¦: {speed:.1f} KB/s")

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆå¯é€‰ï¼‰
            file_hash = self._calculate_file_hash(str(save_path))
            if file_hash:
                self.logger.debug(f"ğŸ”¢ æ–‡ä»¶MD5: {file_hash}")

            return True, str(save_path)

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else "æœªçŸ¥"
            error_msg = f"ä¸‹è½½å¤±è´¥ (HTTP {status_code}): {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            return False, error_msg
        except Exception as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            return False, error_msg
    # çœŸå®ä¸‹è½½æ–‡ä»¶
    def download_pgm(self, pgm, save_dir: str = None):
        pgm_type = self._determine_pgm_type(pgm)
        process_id = pgm.get('processId')
        draft_id = pgm.get('draftId')
        self.logger.info(f"ğŸ“Š å¼€å§‹ä¸‹è½½PGM: {pgm_type} draft_id= {draft_id}, process_id= {process_id}")
        work_squence = pgm.get('workSequence')
        detail = self._get_pgm_detial(pgm_type, process_id, work_squence)
        file_info_list = detail.get('file_info')
        for file in file_info_list:
            file_download_id = file.get('file_download_id')
            file_name = file.get('file_name')
            self._download_sigle_file(file_download_id, file_name, process_id,1, save_dir)
        return detail

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦ï¼ŒåŒæ—¶ä¿æŒéŸ©æ–‡ç­‰Unicodeå­—ç¬¦"""
        if not filename:
            return "unknown_file"

        # ç§»é™¤éæ³•å­—ç¬¦ï¼Œä¿ç•™Unicodeå­—ç¬¦ï¼ˆå¦‚éŸ©æ–‡ï¼‰
        invalid_chars = '<>:"/\\|?*\t\n\r'  # åŒ…æ‹¬æ§åˆ¶å­—ç¬¦
        for char in invalid_chars:
            filename = filename.replace(char, '_')

        # å¤„ç†å¯èƒ½çš„é—®é¢˜å­—ç¬¦ï¼Œä½†ä¿ç•™éŸ©æ–‡å­—æ¯
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰æ§åˆ¶å­—ç¬¦ï¼ˆé™¤å¸¸è§ç©ºç™½å­—ç¬¦å¤–ï¼‰
        filename = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '_', filename)

        # ç¡®ä¿å­—ç¬¦ä¸²æ˜¯æœ‰æ•ˆçš„UTF-8ç¼–ç ï¼Œè¿™å¯¹éŸ©æ–‡ç­‰Unicodeå­—ç¬¦å¾ˆé‡è¦
        try:
            # å…ˆç¼–ç å†è§£ç ä»¥ç¡®ä¿å­—ç¬¦ä¸²çš„æœ‰æ•ˆæ€§
            filename = filename.encode('utf-8').decode('utf-8')
        except UnicodeDecodeError:
            # å¦‚æœé‡åˆ°è§£ç é”™è¯¯ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†ç­–ç•¥
            filename = filename.encode('utf-8', errors='replace').decode('utf-8')

        # é™åˆ¶é•¿åº¦ï¼Œè€ƒè™‘é•¿Unicodeå­—ç¬¦å¯èƒ½çš„å½±å“
        if len(filename) > 255:
            name, ext = os.path.splitext(filename)
            filename = name[:250 - len(ext)] + ext

        return filename

    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes == 0:
            return "0B"

        units = ['B', 'KB', 'MB', 'GB']
        i = 0
        size = float(size_bytes)

        while size >= 1024 and i < len(units) - 1:
            size /= 1024
            i += 1

        return f"{size:.2f} {units[i]}"

    def _calculate_file_hash(self, file_path: str, algorithm: str = 'md5') -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        if not os.path.exists(file_path):
            return ""

        hash_func = hashlib.new(algorithm)

        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b''):
                    hash_func.update(chunk)
            return hash_func.hexdigest()
        except:
            return ""

    def _create_download_summary(self, results: Dict[str, Any], target_dir: str):
        """åˆ›å»ºä¸‹è½½æ‘˜è¦æ–‡ä»¶"""
        try:
            summary_path = Path(target_dir) / "download_summary.json"

            summary = {
                'timestamp': datetime.now().isoformat(),
                'pgm_info': results.get('pgm_info', {}),
                'download_stats': {
                    'successful': len(results.get('downloaded_files', [])),
                    'failed': len(results.get('failed_files', [])),
                    'total_size': results.get('total_size', 0),
                    'total_time': results.get('total_time', 0)
                },
                'downloaded_files': results.get('downloaded_files', []),
                'failed_files': results.get('failed_files', [])
            }

            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)

            self.logger.info(f"ğŸ“ åˆ›å»ºä¸‹è½½æ‘˜è¦: {summary_path}")

        except Exception as e:
            self.logger.warning(f"âš ï¸ åˆ›å»ºä¸‹è½½æ‘˜è¦å¤±è´¥: {str(e)}")
